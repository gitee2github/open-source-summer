{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公共配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTFILE = './EventFile/'\n",
    "OUTPUTFILE = './OutputFile/'\n",
    "\n",
    "TRAINDATA = './OutputFile/trainData.txt'\n",
    "TESTDATA = './OutputFile/testData.txt'\n",
    "TESTDATASIZE = 0.1\n",
    "\n",
    "VOCAB_PATH = './OutputFile/vocab.txt'\n",
    "LABEL_PATH = './OutputFile/label.txt'\n",
    "\n",
    "WORD_PAD = '<PAD>'\n",
    "WORD_UNK = '<UNK>'\n",
    "VOCAB_SIZE = 3000\n",
    "\n",
    "WORD_PAD_ID = 0\n",
    "LABEL_O_ID = 0\n",
    "WORD_UNK_ID = 1\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 256\n",
    "TARGET_SIZE = 3\n",
    "LR = 1e-3\n",
    "EPOCH = 100\n",
    "MODEL_DIR = './OutputFile/model/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得打标签的位置\n",
    "def get_annotation(annPath):\n",
    "    with open(annPath, encoding='utf-8') as file:\n",
    "        anns = {}\n",
    "        for line in file.readlines():\n",
    "            arr = line.split('\\t')[1].split()\n",
    "            name = arr[0]\n",
    "            start = int(arr[1])\n",
    "            end = int(arr[-1])\n",
    "            \n",
    "            # 标注太长显然不正确\n",
    "            if end - start > 50:\n",
    "                continue\n",
    "\n",
    "            anns[start] = 'B-' + name\n",
    "            for i in range(start + 1, end):\n",
    "                anns[i] = 'I-' + name\n",
    "        \n",
    "        return anns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得文件\n",
    "def get_text(textPath):\n",
    "    text = ''\n",
    "    with open(textPath, encoding='utf-8') as file:\n",
    "        for t in file.read():\n",
    "            if t == '\\n':\n",
    "                text += '/n'\n",
    "            elif t == ',':\n",
    "                text += '。'\n",
    "            else:\n",
    "                text += t\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签和文本结合\n",
    "def generate_annotation():\n",
    "    for textPath in glob(EVENTFILE + '*.txt'):\n",
    "        annPath = textPath[:-3] + 'ann'\n",
    "        anns = get_annotation(annPath)\n",
    "        text = get_text(textPath)\n",
    "        # 打标签 先全打O 然后替换\n",
    "        df = pd.DataFrame({'word': list(text), 'label': ['O'] * len(text)})\n",
    "        df.loc[anns.keys(), 'label'] = list(anns.values())\n",
    "        # 导出文件\n",
    "        fileName = os.path.split(textPath)[1]\n",
    "        df.to_csv(OUTPUTFILE + fileName, header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分数据集\n",
    "def get_data():\n",
    "    data = glob(OUTPUTFILE + '*.txt')\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "    n = int(len(data) * TESTDATASIZE)\n",
    "    testData = data[:n]\n",
    "    trainData = data[n:]\n",
    "    merge_file(trainData, TRAINDATA)\n",
    "    merge_file(testData, TESTDATA)\n",
    "\n",
    "\n",
    "# 合并文件\n",
    "def merge_file(data, path):\n",
    "    with open(path, 'a', encoding='utf-8') as file:\n",
    "        for f in data:\n",
    "            text = open(f, encoding='utf-8').read()\n",
    "            file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成词表\n",
    "def generate_vocab():\n",
    "    df = pd.read_csv(TRAINDATA, usecols=[0], names=['word'])\n",
    "    vocal_list = [WORD_PAD, WORD_UNK] + df['word'].value_counts().keys().tolist()\n",
    "    vocal_list = vocal_list[:VOCAB_SIZE]\n",
    "    vocal_dict = {v: k for k, v in enumerate(vocal_list)}\n",
    "    vocal = pd.DataFrame(list(vocal_dict.items()))\n",
    "    vocal.to_csv(VOCAB_PATH, header=None, index=None)\n",
    "\n",
    "\n",
    "# 生成标签表\n",
    "def generate_label():\n",
    "    df = pd.read_csv(TRAINDATA, usecols=[1], names=['label'])\n",
    "    label_list = df['label'].value_counts().keys().tolist()\n",
    "    label_dict = {v: k for k, v in enumerate(label_list)}\n",
    "    label = pd.DataFrame(list(label_dict.items()))\n",
    "    label.to_csv(LABEL_PATH, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_annotation()\n",
    "\n",
    "# get_data()\n",
    "\n",
    "# generate_vocab()\n",
    "# generate_label()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到关系转化表\n",
    "def get_vocab():\n",
    "    df = pd.read_csv(VOCAB_PATH, names=['word', 'id'])\n",
    "    return list(df['word']), dict(df.values)\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    df = pd.read_csv(LABEL_PATH, names=['label', 'id'])\n",
    "    return list(df['label']), dict(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, type='train', base_len=50):\n",
    "        super().__init__()\n",
    "        self.base_len = base_len\n",
    "        data_path = TRAINDATA if type == 'train' else TESTDATA\n",
    "        self.df = pd.read_csv(data_path, encoding='utf-8', names=['word', 'label'])\n",
    "        _, self.word2id = get_vocab()\n",
    "        _, self.label2id = get_label()\n",
    "        self.get_points()\n",
    "\n",
    "\n",
    "    # 找切分点\n",
    "    def get_points(self):\n",
    "        self.points = [0]\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i + self.base_len >= len(self.df):\n",
    "                self.points.append(len(self.df))\n",
    "                break\n",
    "            if self.df.loc[i + self.base_len, 'label'] == 'O':\n",
    "                i += self.base_len\n",
    "                self.points.append(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.points) - 1\n",
    "\n",
    "    \n",
    "    # 向量化\n",
    "    def __getitem__(self, index):\n",
    "        df = self.df[self.points[index] : self.points[index + 1]]\n",
    "        wordUnkId = self.word2id[WORD_UNK]\n",
    "        labelOId = self.label2id['O']\n",
    "        input = [self.word2id.get(w, wordUnkId) for w in df['word']]\n",
    "        target = [self.label2id.get(l, labelOId) for l in df['label']]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充\n",
    "def collate_fn(batch):\n",
    "    # 按句子长度从大到小排序\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    # 拿到最长句子\n",
    "    max_len = len(batch[0][0])\n",
    "    input = []\n",
    "    target = []\n",
    "    mask = []\n",
    "\n",
    "    for item in batch:\n",
    "        pad_len = max_len - len(item[0])\n",
    "        input.append(item[0] + [WORD_PAD_ID] * pad_len)\n",
    "        target.append(item[1] + [LABEL_O_ID] * pad_len)\n",
    "        mask.append([1] * len(item[0]) + [0] * pad_len)\n",
    "    \n",
    "    return torch.tensor(input), torch.tensor(target), torch.tensor(mask).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM, WORD_PAD_ID)\n",
    "        self.lstm = nn.LSTM(\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_SIZE,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        self.linear = nn.Linear(2 * HIDDEN_SIZE, TARGET_SIZE)\n",
    "        self.crf = CRF(TARGET_SIZE)\n",
    "    \n",
    "\n",
    "    def _get_lstm_feature(self, input):\n",
    "        out = self.embed(input)\n",
    "        out, _ = self.lstm(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        out = self._get_lstm_feature(input)\n",
    "        return self.crf.decode(out, mask)\n",
    "\n",
    "    \n",
    "    def loss_fn(self, input, target, mask):\n",
    "        y_pred = self._get_lstm_feature(input)\n",
    "        return -self.crf.forward(y_pred, target, mask, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c0e419a46ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-721105d1edbd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWORD_PAD_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         self.lstm = nn.LSTM(\n\u001b[0;32m      6\u001b[0m             \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fill_padding_idx_with_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    for b, (input, target, mask) in enumerate(loader):\n",
    "        y_pred = model(input, mask)\n",
    "        loss = model.loss_fn(input, target, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 10 == 0:\n",
    "            print(f'Epoch: {e} Loss: {loss.item()}')\n",
    "    if e % 10 == 0:\n",
    "        torch.save(model, MODEL_DIR + f'model_{e}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 19536 accuracy: 0.7291154861450195\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('test')\n",
    "loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = torch.load(MODEL_DIR + 'model_100.pth')\n",
    "\n",
    "    y_ture_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    for b, (input, target, mask) in enumerate(loader):\n",
    "        y_pred = model(input, mask)\n",
    "        # loss = model.loss_fn(input, target, mask)\n",
    "\n",
    "        for lst in y_pred:\n",
    "            y_pred_list += lst\n",
    "\n",
    "        for y, m in zip(target, mask):\n",
    "            y_ture_list += y[m == True].tolist()\n",
    "\n",
    "    y_ture_tensor = torch.tensor(y_ture_list)\n",
    "    y_pred_tensor = torch.tensor(y_pred_list)\n",
    "\n",
    "    accuracy = (y_ture_tensor == y_pred_tensor).sum() / len(y_ture_tensor)\n",
    "    print(f'total: {len(y_ture_tensor)} accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(label, text):\n",
    "    i = 0\n",
    "    res = []\n",
    "    while i < len(label):\n",
    "        if label[i] != 'O':\n",
    "            prefix, name = label[i].split('-')\n",
    "            start = end = i\n",
    "            i += 1\n",
    "            while i < len(label) and label[i] == 'I-' + name:\n",
    "                end = i\n",
    "                i += 1\n",
    "            \n",
    "            res.append(text[start:end + 1])\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户添加\n",
    "def userAdd(text, arr):\n",
    "    userWord = []\n",
    "    with open('./userDict.txt', encoding='utf-8') as file:\n",
    "        for l in file.readlines():\n",
    "            userWord.append(l.split('\\n')[0])\n",
    "\n",
    "    for w in userWord:\n",
    "        if w in text:\n",
    "            arr.append(w)\n",
    "\n",
    "    return arr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Triple 协议', '\\n', 'Dubbo QoS', 'RPC 协议', 'Dubbo', '网络通信原理', 'Netty', 'Qos 协议', 'Netty Server', 'Java', 'rpc 框架']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "一个端口支持多种协议可以使部署和运维更为方便，甚至在一些特殊的开发场景也能降低复杂度。\n",
    "1、对于协议切换/架构升级的场景，通常需要同时暴露多个协议，单端口多协议能最大程度上带来运维的便捷性。\n",
    "2、从后端开发的角度一套业务代码加少许配置就能暴露多个端口也能带来降低开发成本 目前 Dubbo 支持一个应用对外发布多种 RPC 协议，但这些 RPC 协议都需要独立占用一个服务端端口，另外 Dubbo QoS 也同样占用了一个端口。维护这些端口的监听需要消耗一定的资源，同时暴露多端口对于运维也存在一定复杂度，如 VIP /域名等。因此可以通过在同一个端口支持多种复用协议来降低复杂度，提高易用性\n",
    "例如：一个业务逻辑需要提供给不同语言、不同业务方进行调用。 \n",
    "当前大多数 rpc 框架均不支持该逻辑（包括 Dubbo）以 Dubbo 举例，使用Triple 协议并开启所有默认服务，会开启如下默认端口 Triple: 50051、Metadata : 20880、Qos: 22222。如果后续增加其他功能可能还会更多，这很不优雅，并且还启动了多个 Netty Server，造成了资源浪费。 如果本地需要测试，在不修改配置的情况下端口会冲突导致启动失败，体验很差。 \n",
    "\n",
    "实现服务端同端口多协议暴露,将各种协议服务使用一种统一的方式使用同一个 Netty Server 进行暴露\n",
    "将Qos 协议和 Triple 协议使用同一个端口暴露\n",
    "将 Dubbo 的其他协议进行接入\n",
    "\n",
    "熟练使用 Java\n",
    "了解基础网络通信原理\n",
    "了解Netty工作原理\n",
    "\"\"\"\n",
    "_, word2id = get_vocab()\n",
    "\n",
    "input = torch.tensor([[word2id.get(w, WORD_PAD_ID) for w in text]])\n",
    "mask = torch.tensor([[1] * len(text)]).bool()\n",
    "\n",
    "model = torch.load(MODEL_DIR + 'model_90.pth')\n",
    "y_pred = model(input, mask)\n",
    "\n",
    "id2label, _ = get_label()\n",
    "\n",
    "label = []\n",
    "for l in y_pred:\n",
    "    label.append(id2label[l[0]])\n",
    "\n",
    "res = extract(label, text)\n",
    "res = list(set(res))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
