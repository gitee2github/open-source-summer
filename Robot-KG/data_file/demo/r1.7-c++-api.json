{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-converter.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_converter.html", "text_entry": "['mindspore::converter\\n', '\\n', '\\n', '以下描述了Mindspore Lite转换支持的模型类型及用户扩展所需的必要信息。\\n', '\\n', '接口汇总\\n', '类名\\t描述\\n', 'FmkType\\tMindspore Lite支持的框架类型。\\n', 'ConverterParameters\\t模型解析时的只读参数。\\n', 'ConverterContext\\t模型转换时的基本信息设置与获取。\\n', 'NodeParser\\top节点的解析基类。\\n', 'ModelParser\\t模型解析的基类。\\n', 'FmkType\\n', '#include <converter_context.h>\\n', '\\n', 'enum类型变量，定义MindSpore Lite转换支持的框架类型。\\n', '\\n', '类型定义\\t值\\t描述\\n', 'kFmkTypeTf\\t0\\t表示tensorflow框架。\\n', 'kFmkTypeCaffe\\t1\\t表示caffe框架。\\n', 'kFmkTypeOnnx\\t2\\t表示onnx框架。\\n', 'kFmkTypeMs\\t3\\t表示mindspore框架。\\n', 'kFmkTypeTflite\\t4\\t表示tflite框架。\\n', 'ConverterParameters\\n', '#include <converter_context.h>\\n', '\\n', 'struct类型结构体，定义模型解析时的转换参数，用于模型解析时的只读参数。\\n', '\\n', 'struct ConverterParameters {\\n', '  FmkType fmk;                                   // 框架类型\\n', '  std::string model_file;                        // 原始模型文件路径\\n', '  std::string weight_file;                       // 原始模型权重文件路径，仅在Caffe框架下有效\\n', '  std::map<std::string, std::string> attrs;      // 预留参数接口，暂未启用\\n', '};\\n', 'ConverterContext\\n', '#include <converter_context.h>\\n', '\\n', '模型转换过程中，基本信息的设置与获取。\\n', '\\n', 'ConverterContext\\n', 'ConverterContext() = default;\\n', '构造函数。\\n', '\\n', '~ConverterContext\\n', '~ConverterContext() = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'SetGraphOutputTensorNames\\n', 'static void SetGraphOutputTensorNames(const std::vector<std::string> &output_names);\\n', '静态方法，设置导出模型的输出名称。\\n', '\\n', '参数\\n', '\\n', 'output_names: 模型的输出名称。\\n', '\\n', 'GetGraphOutputTensorNames\\n', 'static std::vector<std::string> GetGraphOutputTensorNames();\\n', '静态方法，获取模型的输出名称。\\n', '\\n', '返回值\\n', '\\n', '模型的输出名称，默认与原始模型的输出名称一致。\\n', '\\n', 'NodeParser\\n', '#include <node_parser.h>\\n', '\\n', 'op节点的解析基类。\\n', '\\n', 'NodeParser\\n', 'NodeParser() = default;\\n', '构造函数。\\n', '\\n', '~NodeParser\\n', 'virtual ~NodeParser() = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'Parse\\n', 'ops::PrimitiveC *Parse(const onnx::GraphProto &onnx_graph, const onnx::NodeProto &onnx_node);\\n', 'onnx节点解析接口函数。\\n', '\\n', '参数\\n', '\\n', 'onnx_graph: 模型结构，包含模型的所有信息。\\n', '\\n', 'onnx_node: 待解析节点。\\n', '\\n', '返回值\\n', '\\n', 'PrimitiveC类指针对象，存储节点属性。\\n', '\\n', 'Parse\\n', 'ops::PrimitiveC *Parse(const caffe::LayerParameter &proto, const caffe::LayerParameter &weight);\\n', 'caffe节点解析接口函数。\\n', '\\n', '参数\\n', '\\n', 'proto: 待解析节点，包含节点的属性信息。\\n', '\\n', 'weight: 待解析节点的权重信息。\\n', '\\n', '返回值\\n', '\\n', 'PrimitiveC类指针对象，存储节点属性。\\n', '\\n', 'Parse\\n', 'ops::PrimitiveC *Parse(const tensorflow::NodeDef &tf_op,\\n', '                       const std::map<std::string, const tensorflow::NodeDef *> &tf_node_map,\\n', '                       std::vector<std::string> *inputs, int *output_size);\\n', 'tf节点解析接口函数。\\n', '\\n', '参数\\n', '\\n', 'tf_op: 待解析节点。\\n', '\\n', 'tf_node_map: 模型的所有节点信息。\\n', '\\n', 'inputs: 用户指定当前节点需要哪些原始输入，及其解析后的输入顺序。\\n', '\\n', 'output_size: 用户指定当前节点的输出个数。\\n', '\\n', '返回值\\n', '\\n', 'PrimitiveC类指针对象，存储节点属性。\\n', '\\n', 'Parse\\n', 'ops::PrimitiveC *Parse(const std::unique_ptr<tflite::OperatorT> &tflite_op,\\n', '                       const std::unique_ptr<tflite::ModelT> &tflite_model);\\n', 'tflite节点解析接口函数。\\n', '\\n', '参数\\n', '\\n', 'tflite_op: 待解析节点，包含节点的属性信息。\\n', '\\n', 'tflite_model: 模型结构，包含模型的所有信息。\\n', '\\n', '返回值\\n', '\\n', 'PrimitiveC类指针对象，存储节点属性。\\n', '\\n', 'NodeParserPtr\\n', '#include <node_parser.h>\\n', '\\n', 'NodeParser类的共享智能指针类型。\\n', '\\n', 'using NodeParserPtr = std::shared_ptr<NodeParser>;\\n', 'ModelParser\\n', '#include <model_parser.h>\\n', '\\n', '解析原始模型的基类。\\n', '\\n', 'ModelParser\\n', 'ModelParser() = default;\\n', '构造函数。\\n', '\\n', '~ModelParser\\n', 'virtual ~ModelParser() = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'Parse\\n', 'api::FuncGraphPtr Parse(const converter::ConverterParameters &flags);\\n', '模型解析接口。\\n', '\\n', '参数\\n', '\\n', 'flags: 解析模型时基本信息，具体见ConverterParameters。\\n', '\\n', '返回值\\n', '\\n', 'FuncGraph的共享智能指针。\\n', '\\n', '保护数据成员\\n', 'res_graph_\\n', 'api::FuncGraphPtr res_graph_ = nullptr;\\n', 'FuncGraph的共享智能指针。\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-dataset-config.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_dataset_config.html", "text_entry": "['mindspore::dataset::config\\n', '函数\\n', 'Function mindspore::dataset::config::get_callback_timeout\\n', '\\n', 'Function mindspore::dataset::config::get_monitor_sampling_interval\\n', '\\n', 'Function mindspore::dataset::config::get_num_parallel_workers\\n', '\\n', 'Function mindspore::dataset::config::get_prefetch_size\\n', '\\n', 'Function mindspore::dataset::config::get_seed\\n', '\\n', 'Function mindspore::dataset::config::load\\n', '\\n', 'Function mindspore::dataset::config::load (overloaded)\\n', '\\n', 'Function mindspore::dataset::config::set_callback_timeout\\n', '\\n', 'Function mindspore::dataset::config::set_monitor_sampling_interval\\n', '\\n', 'Function mindspore::dataset::config::set_num_parallel_workers\\n', '\\n', 'Function mindspore::dataset::config::set_prefetch_size\\n', '\\n', 'Function mindspore::dataset::config::set_seed\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-dataset-text.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_dataset_text.html", "text_entry": "['mindspore::dataset::text\\n', '类\\n', 'Class BasicTokenizer\\n', '\\n', 'Class BertTokenizer\\n', '\\n', 'Class CaseFold\\n', '\\n', 'Class JiebaTokenizer\\n', '\\n', 'Class Lookup\\n', '\\n', 'Class Ngram\\n', '\\n', 'Class NormalizeUTF8\\n', '\\n', 'Class RegexReplace\\n', '\\n', 'Class RegexTokenizer\\n', '\\n', 'Class SentencePieceTokenizer\\n', '\\n', 'Class SlidingWindow\\n', '\\n', 'Class ToNumber\\n', '\\n', 'Class TruncateSequencePair\\n', '\\n', 'Class UnicodeCharTokenizer\\n', '\\n', 'Class UnicodeScriptTokenizer\\n', '\\n', 'Class WhitespaceTokenizer\\n', '\\n', 'Class WordpieceTokenizer\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-dataset-transforms.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_dataset_transforms.html", "text_entry": "['mindspore::dataset::transforms\\n', '类\\n', 'Class Compose\\n', '\\n', 'Class Concatenate\\n', '\\n', 'Class Duplicate\\n', '\\n', 'Class Fill\\n', '\\n', 'Class Mask\\n', '\\n', 'Class OneHot\\n', '\\n', 'Class PadEnd\\n', '\\n', 'Class RandomApply\\n', '\\n', 'Class RandomChoice\\n', '\\n', 'Class Slice\\n', '\\n', 'Class TypeCast\\n', '\\n', 'Class Unique\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-dataset-vision.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_dataset_vision.html", "text_entry": "['mindspore::dataset::vision\\n', '类\\n', 'Class AdjustGamma\\n', '\\n', 'Class Affine\\n', '\\n', 'Class AutoContrast\\n', '\\n', 'Class BoundingBoxAugment\\n', '\\n', 'Class CenterCrop\\n', '\\n', 'Class ConvertColor\\n', '\\n', 'Class Crop\\n', '\\n', 'Class CutMixBatch\\n', '\\n', 'Class CutOut\\n', '\\n', 'Class Decode\\n', '\\n', 'Class DvppDecodePng\\n', '\\n', 'Class DvppDecodeResizeCropJpeg\\n', '\\n', 'Class DvppDecodeResizeJpeg\\n', '\\n', 'Class Equalize\\n', '\\n', 'Class GaussianBlur\\n', '\\n', 'Class HorizontalFlip\\n', '\\n', 'Class HWC2CHW\\n', '\\n', 'Class Invert\\n', '\\n', 'Class MixUpBatch\\n', '\\n', 'Class Normalize\\n', '\\n', 'Class NormalizePad\\n', '\\n', 'Class Pad\\n', '\\n', 'Class RandomAffine\\n', '\\n', 'Class RandomColor\\n', '\\n', 'Class RandomColorAdjust\\n', '\\n', 'Class RandomCrop\\n', '\\n', 'Class RandomCropDecodeResize\\n', '\\n', 'Class RandomCropWithBBox\\n', '\\n', 'Class RandomHorizontalFlip\\n', '\\n', 'Class RandomHorizontalFlipWithBBox\\n', '\\n', 'Class RandomPosterize\\n', '\\n', 'Class RandomResize\\n', '\\n', 'Class RandomResizedCrop\\n', '\\n', 'Class RandomResizedCropWithBBox\\n', '\\n', 'Class RandomResizeWithBBox\\n', '\\n', 'Class RandomRotation\\n', '\\n', 'Class RandomSelectSubpolicy\\n', '\\n', 'Class RandomSharpness\\n', '\\n', 'Class RandomSolarize\\n', '\\n', 'Class RandomVerticalFlip\\n', '\\n', 'Class RandomVerticalFlipWithBBox\\n', '\\n', 'Class Rescale\\n', '\\n', 'Class Resize\\n', '\\n', 'Class ResizePreserveAR\\n', '\\n', 'Class ResizeWithBBox\\n', '\\n', 'Class RGB2BGR\\n', '\\n', 'Class RGB2GRAY\\n', '\\n', 'Class RGBA2BGR\\n', '\\n', 'Class RGBA2RGB\\n', '\\n', 'Class Rotate\\n', '\\n', 'Class SlicePatches\\n', '\\n', 'Class SoftDvppDecodeRandomCropResizeJpeg\\n', '\\n', 'Class SoftDvppDecodeResizeJpeg\\n', '\\n', 'Class SwapRedBlue\\n', '\\n', 'Class UniformAugment\\n', '\\n', 'Class VerticalFlip']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-dataset.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_dataset.html", "text_entry": "['mindspore::dataset\\n', 'Dataset函数\\n', 'Vision\\n', 'API\\n', '\\n', '重载的API（重载的Sampler）\\n', '\\n', 'Function mindspore::dataset::Album\\n', '\\n', 'Function mindspore::dataset::Album (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Album (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::CelebA\\n', '\\n', 'Function mindspore::dataset::CelebA (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::CelebA (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::Cifar100\\n', '\\n', 'Function mindspore::dataset::Cifar100 (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Cifar100 (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::Cifar10\\n', '\\n', 'Function mindspore::dataset::Cifar10 (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Cifar10 (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::Coco\\n', '\\n', 'Function mindspore::dataset::Coco (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Coco (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::ImageFolder\\n', '\\n', 'Function mindspore::dataset::ImageFolder (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::ImageFolder (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::Mnist\\n', '\\n', 'Function mindspore::dataset::Mnist (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Mnist (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::VOC\\n', '\\n', 'Function mindspore::dataset::VOC (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::VOC (reference Sampler)\\n', '\\n', '文本\\n', 'API\\n', '\\n', '重载的API\\n', '\\n', 'Function mindspore::dataset::CLUE\\n', '\\n', '无\\n', '\\n', '标准形式\\n', 'API\\n', '\\n', '重载的API（重载的Sampler）\\n', '\\n', 'Function mindspore::dataset::CSV\\n', '\\n', '无\\n', '\\n', 'Function mindspore::dataset::Manifest\\n', '\\n', 'Function mindspore::dataset::Manifest (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::Manifest (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::MindData\\n', '\\n', 'Function mindspore::dataset::MindData (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::MindData (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::MindData\\n', '\\n', 'Function mindspore::dataset::MindData (raw ptr Sampler)\\n', '\\n', 'Function mindspore::dataset::MindData (reference Sampler)\\n', '\\n', 'Function mindspore::dataset::RandomData\\n', '\\n', '无\\n', '\\n', 'Function mindspore::dataset::TextFile\\n', '\\n', '无\\n', '\\n', 'Function mindspore::dataset::TFRecord\\n', '\\n', '无\\n', '\\n', 'Dataset类\\n', 'Class Dataset\\n', '\\n', 'Class Iterator\\n', '\\n', 'Class PullIterator\\n', '\\n', 'Sampler类\\n', 'Class Sampler\\n', '\\n', 'Class DistributedSampler\\n', '\\n', 'Class PKSampler\\n', '\\n', 'Class RandomSampler\\n', '\\n', 'Class SequentialSampler\\n', '\\n', 'Class SubsetRandomSampler\\n', '\\n', 'Class SubsetSampler\\n', '\\n', 'Class WeightedRandomSampler\\n', '\\n', 'Eager类\\n', 'Class Execute\\n', '\\n', '常量\\n', 'Enum BorderType\\n', '\\n', 'Enum ImageBatchFormat\\n', '\\n', 'Enum ImageFormat\\n', '\\n', 'Enum InterpolationMode\\n', '\\n', 'Enum JiebaMode\\n', '\\n', 'Enum MapTargetDevice\\n', '\\n', 'Enum NormalizeForm\\n', '\\n', 'Enum RelationalOp\\n', '\\n', 'Enum SamplingStrategy\\n', '\\n', 'Enum SentencePieceModel\\n', '\\n', 'Enum ShuffleMode\\n', '\\n', 'Enum SPieceTokenizerLoadType\\n', '\\n', 'Enum SPieceTokenizerOutType\\n', '\\n', 'Enum TensorImpl\\n', '\\n', 'Variable mindspore::dataset::kCfgCallbackTimeout\\n', '\\n', 'Variable mindspore::dataset::kCfgDefaultCacheHost\\n', '\\n', 'Variable mindspore::dataset::kCfgDefaultCachePort\\n', '\\n', 'Variable mindspore::dataset::kCfgDefaultRankId\\n', '\\n', 'Variable mindspore::dataset::kCfgDefaultSeed\\n', '\\n', 'Variable mindspore::dataset::kCfgMonitorSamplingInterval\\n', '\\n', 'Variable mindspore::dataset::kCfgOpConnectorSize\\n', '\\n', 'Variable mindspore::dataset::kCfgParallelWorkers\\n', '\\n', 'Variable mindspore::dataset::kCfgRowsPerBuffer\\n', '\\n', 'Variable mindspore::dataset::kCfgWorkerConnectorSize\\n', '\\n', 'Variable mindspore::dataset::kCVInvalidType\\n', '\\n', 'Variable mindspore::dataset::kDecimal\\n', '\\n', 'Variable mindspore::dataset::kDeMaxDim\\n', '\\n', 'Variable mindspore::dataset::kDeMaxFreq\\n', '\\n', 'Variable mindspore::dataset::kDeMaxRank\\n', '\\n', 'Variable mindspore::dataset::kDeMaxTopk\\n', '\\n', 'Variable mindspore::dataset::kDftAutoNumWorkers\\n', '\\n', 'Variable mindspore::dataset::kDftMetaColumnPrefix\\n', '\\n', 'Variable mindspore::dataset::kDftNumConnections\\n', '\\n', 'Variable mindspore::dataset::kMaxLegalPort\\n', '\\n', 'Variable mindspore::dataset::kMinLegalPort\\n', '\\n', '其他\\n', '类\\n', 'Class SentencePieceVocab\\n', '\\n', 'Class Slice\\n', '\\n', 'Class SliceOption\\n', '\\n', 'Class TensorTransform\\n', '\\n', 'Class Vocab\\n', '\\n', '函数\\n', 'Function mindspore::dataset::Schema\\n', '\\n', 'Function mindspore::dataset::SchemaCharIF\\n', '\\n', 'Function mindspore::dataset::CreateDatasetCache\\n', '\\n', 'Function mindspore::dataset::CreateDatasetCacheCharIF\\n', '\\n', '定义类型\\n', 'Typedef mindspore::dataset::connection_id_type\\n', '\\n', 'Typedef mindspore::dataset::dsize_t\\n', '\\n', 'Typedef mindspore::dataset::MSTensorMap\\n', '\\n', 'Typedef mindspore::dataset::MSTensorMapChar\\n', '\\n', 'Typedef mindspore::dataset::MSTensorVec\\n', '\\n', 'Typedef mindspore::dataset::row_id_type\\n', '\\n', 'Typedef mindspore::dataset::session_id_type\\n', '\\n', 'Typedef mindspore::dataset::uchar\\n', '\\n', 'Lite-CV\\n', '类\\n', 'Class LiteMat\\n', '\\n', 'Class LDataType\\n', '\\n', '函数\\n', 'Function mindspore::dataset::Affine\\n', '\\n', 'Function mindspore::dataset::Affine\\n', '\\n', 'Function mindspore::dataset::ApplyNms\\n', '\\n', 'Function mindspore::dataset::Canny\\n', '\\n', 'Function mindspore::dataset::Conv2D\\n', '\\n', 'Function mindspore::dataset::ConvertRgbToGray\\n', '\\n', 'Function mindspore::dataset::ConvertTo\\n', '\\n', 'Function mindspore::dataset::Crop\\n', '\\n', 'Function mindspore::dataset::Divide\\n', '\\n', 'Function mindspore::dataset::ExtractChannel\\n', '\\n', 'Function mindspore::dataset::GaussianBlur\\n', '\\n', 'Function mindspore::dataset::GetAffineTransform\\n', '\\n', 'Function mindspore::dataset::GetPerspectiveTransform\\n', '\\n', 'Function mindspore::dataset::GetRotationMatrix2D\\n', '\\n', 'Function mindspore::dataset::HWC2CHW\\n', '\\n', 'Function mindspore::dataset::InitFromPixel\\n', '\\n', 'Function mindspore::dataset::Merge\\n', '\\n', 'Function mindspore::dataset::Multiply\\n', '\\n', 'Function mindspore::dataset::Pad\\n', '\\n', 'Function mindspore::dataset::ResizeBilinear\\n', '\\n', 'Function mindspore::dataset::ResizePreserveARWithFiller\\n', '\\n', 'Function mindspore::dataset::Sobel\\n', '\\n', 'Function mindspore::dataset::Split\\n', '\\n', 'Function mindspore::dataset::SubStractMeanNormalize\\n', '\\n', 'Function mindspore::dataset::Subtract\\n', '\\n', 'Function mindspore::dataset::Transpose\\n', '\\n', 'Function mindspore::dataset::WarpAffineBilinear\\n', '\\n', 'Function mindspore::dataset::WarpPerspectiveBilinear\\n', '\\n', '常量 / 结构体\\n', 'Enum PaddBorderType\\n', '\\n', 'Struct Point']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-kernel.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_kernel.html", "text_entry": "['mindspore::kernel\\n', '\\n', '\\n', '接口汇总\\n', '类名\\t描述\\n', 'Kernel\\t算子基类。\\n', 'KernelInterface\\t算子扩展能力基类。\\n', 'Kernel\\n', '#include <kernel.h>\\n', '\\n', 'Kernel是算子实现的基类，定义了几个必须实现的接口。\\n', '\\n', '构造函数\\n', 'Kernel\\n', 'Kernel()\\n', '\\n', 'Kernel(const std::vector<mindspore::MSTensor> &inputs, const std::vector<mindspore::MSTensor> &outputs,\\n', '       const schema::Primitive *primitive, const mindspore::Context *ctx)\\n', 'Kernel的默认与带参构造函数，构造Kernel实例。\\n', '\\n', '参数\\n', '\\n', 'inputs: 算子输入MSTensor。\\n', '\\n', 'outputs: 算子输出MSTensor。\\n', '\\n', 'primitive: 算子经由flatbuffers反序化为Primitive后的结果。\\n', '\\n', 'ctx: 算子的上下文Context。\\n', '\\n', '析构函数\\n', '~Kernel\\n', 'virtual ~Kernel()\\n', 'Kernel的析构函数。\\n', '\\n', '公有成员函数\\n', 'Prepare\\n', 'virtual int Prepare()\\n', '进行算子运行前相关的准备工作，MindSpore Lite 框架运行时会对所有算子执行一遍Prepare后再执行Execute。\\n', '\\n', 'Execute\\n', 'virtual int Execute()\\n', '运行算子。\\n', '\\n', 'ReSize\\n', 'virtual int ReSize()\\n', '在用户调用Model::Resize接口时，或是模型推理中需要重新推理算子形状时，会调用到该接口。 在ReSize函数中，若有必要，根据输入的形状态重新推理输出形状，并分配算子运算中需要的内存。\\n', '\\n', 'type\\n', 'virtual schema::PrimitiveType type()\\n', '返回算子的类型。\\n', '\\n', 'quant_type\\n', 'virtual schema::QuantType quant_type()\\n', '返回算子的量化类型。\\n', '\\n', 'set_inputs\\n', 'virtual void set_inputs(const std::vector<mindspore::MSTensor> &in_tensors)\\n', '设置算子的输入列表。\\n', '\\n', '参数\\n', '\\n', 'in_tensors: 算子的所有输入MSTensor列表。\\n', '\\n', 'set_input\\n', 'virtual set_input(mindspore::MSTensor in_tensor, int index)\\n', '设置算子指定位置的输入。\\n', '\\n', '参数\\n', '\\n', 'in_tensor: 算子的输入MSTensor。\\n', '\\n', 'index: 算子输入在所有输入中的下标，从0开始计数。\\n', '\\n', 'set_outputs\\n', 'virtual void set_outputs(const std::vector<mindspore::MSTensor> &out_tensors)\\n', '设置算子的输出列表。\\n', '\\n', '参数\\n', '\\n', 'out_tensor: 算子的所有输出MSTensor列表。\\n', '\\n', 'set_output\\n', 'virtual void set_output(mindspore::MSTensor out_tensor, int index)\\n', '设置算子指定位置的输出。\\n', '\\n', '参数\\n', '\\n', 'out_tensor: 算子的输出MSTensor。\\n', '\\n', 'index: 算子输出在所有输出中的下标，从0开始计数。\\n', '\\n', 'inputs\\n', 'virtual const std::vector<mindspore::MSTensor *> &inputs()\\n', '返回算子的所有输入MSTensor列表。\\n', '\\n', 'outputs\\n', 'virtual const std::vector<mindspore::MSTensor *> &outputs()\\n', '返回算子的所有输出MSTensor列表。\\n', '\\n', 'name\\n', 'std::string name()\\n', '返回算子的名称。\\n', '\\n', 'set_name\\n', 'void set_name(const std::string &name)\\n', '设置算子的名称。\\n', '\\n', '参数\\n', '\\n', 'name: 算子名称。\\n', '\\n', 'context\\n', 'const lite::Context *context() const\\n', '返回算子对应的Context。\\n', '\\n', 'primitive\\n', 'const schema::Primitive *primitive() const\\n', '返回算子经由flatbuffers反序化为Primitive后的结果。\\n', '\\n', 'GetAttr\\n', 'std::string GetAttr(const std::string &key) const\\n', '获取指定配置名对应的配置。\\n', '\\n', '参数\\n', '\\n', 'key: 配置名。\\n', '\\n', 'SetConfig\\n', 'void SetConfig(const std::map<std::string, std::map<std::string, std::string>> *config)\\n', '保存配置内容的常量指针到kernel里，该接口当前是由框架在加载配置文件时自动触发调用的，不建议用户使用。\\n', '\\n', '参数\\n', '\\n', 'config: 配置的常量指针。\\n', '\\n', 'GetConfig\\n', 'std::map<std::string, std::string> GetConfig(const std::string &section) const\\n', '获取指定章节名对应的配置。\\n', '\\n', '参数\\n', '\\n', 'section: 配置的章节名称。\\n', '\\n', 'KernelInterface\\n', '#include <kernel_interface.h>\\n', '\\n', '算子扩展能力基类。\\n', '\\n', '~KernelInterface\\n', 'virtual ~KernelInterface()\\n', '析构函数。\\n', '\\n', 'KernelInterfaceCreator\\n', 'using KernelInterfaceCreator = std::function<std::shared_ptr<KernelInterface>()>\\n', '创建KernelInterface的函数原型声明。\\n', '\\n', '公有成员函数\\n', 'Infer\\n', '算子的InferShape能力，用于根据输入推导出输出的形状、数据类型以及format。\\n', '\\n', 'virtual int Infer(std::vector<mindspore::MSTensor> *inputs, std::vector<mindspore::MSTensor> *outputs, const schema::Primitive *primitive, const Kernel *kernel)\\n', '参数\\n', '\\n', 'inputs: 算子输入MSTensor。\\n', '\\n', 'outputs: 算子输出MSTensor。\\n', '\\n', 'primitive: 算子经过flatbuffers反序化后的结果，存储算子属性。\\n', '\\n', 'kernel: 算子的基类结构，在Build阶段，kernel是空指针，Build完成后框架传递的kernel才有值，当kernel非空时就不建议去操作primitive了，因为有可能primtive已经无效了。\\n', '\\n', 'Infer\\n', '算子的InferShape能力，用于根据输入推导出输出的shape、数据类型以及format。\\n', '\\n', '该接口已不推荐使用，建议使用带有kernel参数的Infer接口。因为如果模型通过以下Build接口执行编译，编译后框架会自动释放模型的内存，导致primitive不可用。\\n', '\\n', 'Status Build(GraphCell graph, const std::shared_ptr<Context> &model_context = nullptr,\\n', '               const std::shared_ptr<TrainCfg> &train_cfg = nullptr)\\n', 'virtual int Infer(std::vector<mindspore::MSTensor> *inputs, std::vector<mindspore::MSTensor> *outputs, const schema::Primitive *primitive)\\n', '参数\\n', '\\n', 'inputs: 算子输入MSTensor。\\n', '\\n', 'outputs: 算子输出MSTensor。\\n', '\\n', 'primitive: 算子经过flatbuffers反序化后的结果，存储算子属性。\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-registry-opencl.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_registry_opencl.html", "text_entry": "['mindspore::registry::opencl\\n', '\\n', '\\n', '接口汇总\\n', '类名\\t描述\\n', 'OpenCLRuntimeWrapper\\t端侧GPU操作的接口类\\n', 'OpenCLRuntimeWrapper\\n', '#include <include/registry/opencl_runtime_wrapper.h>\\n', '\\n', 'OpenCLRuntimeWrapper类包装了内部OpenCL的相关接口，用于支持南向GPU算子的开发。\\n', '\\n', 'OpenCLRuntimeWrapper\\n', 'OpenCLRuntimeWrapper() = default;\\n', '构造函数。\\n', '\\n', '~OpenCLRuntimeWrapper\\n', '~OpenCLRuntimeWrapper = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'LoadSource\\n', 'Status LoadSource(const std::string &program_name, const std::string &source);\\n', '加载OpenCL源代码并指定程序名。\\n', '\\n', '参数\\n', '\\n', 'program_name: OpenCL源程序名称。\\n', '\\n', 'source: OpenCL源程序。\\n', '\\n', 'BuildKernel\\n', 'Status BuildKernel(cl::Kernel *kernel, const std::string &program_name, const std::string &kernel_name,\\n', '                     const std::vector<std::string> &build_options_ext = {});\\n', '构建OpenCL代码。\\n', '\\n', '参数\\n', '\\n', 'kernel: 用于返回已编译的内核。\\n', '\\n', 'program_name: OpenCL源程序名称。\\n', '\\n', 'kernel_name: OpenCL内核名称。\\n', '\\n', 'build_options_ext: OpenCL内核构建选项。\\n', '\\n', 'SetKernelArg\\n', 'Status SetKernelArg(const cl::Kernel &kernel, uint32_t index, void *const value);\\n', '设置OpenCL内核运行时指针类参数的值。\\n', '\\n', '参数\\n', '\\n', 'kernel: OpenCL内核。\\n', '\\n', 'index: OpenCL内核参数索引。\\n', '\\n', 'value: OpenCL内核参数值指针。\\n', '\\n', 'template <typename T>\\n', '  typename std::enable_if<!std::is_pointer<T>::value, Status>::type SetKernelArg(const cl::Kernel &kernel,\\n', '                                                                                 uint32_t index, const T value);\\n', '设置OpenCL内核运行时非指针类参数的值。\\n', '\\n', '参数\\n', '\\n', 'kernel: OpenCL内核。\\n', '\\n', 'index: OpenCL内核参数索引。\\n', '\\n', 'value: OpenCL内核参数值。\\n', '\\n', 'RunKernel\\n', 'Status RunKernel(const cl::Kernel &kernbel, const cl::NDRange &global, const cl::NDRange &local);\\n', '运行OpenCL内核。\\n', '\\n', '参数\\n', '\\n', 'kernel: OpenCL内核。\\n', '\\n', 'global: 定义工作项的总数量。\\n', '\\n', 'local: 定义每个工作组中工作项的数量。\\n', '\\n', 'SyncCommandQueue\\n', 'Status SyncCommandQueue();\\n', '同步指令队列。\\n', '\\n', 'GetAllocator\\n', 'std::shared_ptr<Allocator> GetAllocator();\\n', '获取GPU内存分配器的智能指针。通过Allocator接口，可申请GPU内存，用于OpenCL内核的运算。\\n', '\\n', 'MapBuffer\\n', 'void *MapBuffer(void *host_ptr, int flags, bool sync = true);\\n', '重新将GPU内存映射到主机内存地址，以便读写。\\n', '\\n', '参数\\n', '\\n', 'host_ptr: 主机内存地址（为GPU内存所映射的CPU地址）。\\n', '\\n', 'flags: 内存映射的OpenCL功能符号，如CL_MAP_READ，CL_MAP_WRITE。\\n', '\\n', 'sync: 是否同步标志。\\n', '\\n', 'UnmapBuffer\\n', 'Status UnmapBuffer(void *host_ptr);\\n', '将改变后的内存数据，写入GPU。\\n', '\\n', '参数\\n', '\\n', 'host_ptr: 主机内存地址（为GPU内存所映射的CPU地址）。\\n', '\\n', 'ReadImage\\n', 'Status ReadImage(void *buffer, void *dst_data);\\n', '读取解析Image形式的GPU内存到目标地址，写入的数据格式为NHWC4（C轴4数据对齐的NHWC格式数据）。\\n', '\\n', '参数\\n', '\\n', 'buffer: Image格式的GPU内存所映射的主机内存地址。\\n', '\\n', 'dst_data: 目标地址。\\n', '\\n', 'WriteImage\\n', 'Status WriteImage(void *buffer, void *src_data);\\n', '从源地址src_data读取数据，写入到Image形式的GPU内存buffer。\\n', '\\n', '参数\\n', '\\n', 'buffer: Image格式的GPU内存所映射的主机内存地址。\\n', '\\n', 'src_data: 源地址。\\n', '\\n', 'DeviceMaxWorkGroupSize\\n', 'uint64_t DeviceMaxWorkGroupSize();\\n', '获取支持的最大工作组数量。\\n', '\\n', 'GetMaxImage2DWidth\\n', 'uint64_t GetMaxImage2DWidth();\\n', '获取Image内存数据支持的最大宽度。\\n', '\\n', 'GetMaxImage2DHeight\\n', 'uint64_t GetMaxImage2DHeight();\\n', '获取Image内存数据支持的最大高度。\\n', '\\n', 'GetMaxImage2DWidth\\n', 'uint64_t GetImagePitchAlignment();\\n', '获取Image内存数据的宽度对齐值。\\n', '\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore-registry.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore_registry.html", "text_entry": "['mindspore::registry\\n', '\\n', '\\n', '接口汇总\\n', '类名\\t描述\\n', 'NodeParserRegistry\\t扩展Node解析的注册类。\\n', 'REG_NODE_PARSER\\t注册扩展Node解析。\\n', 'ModelParserRegistry\\t扩展Model解析的注册类。\\n', 'REG_MODEL_PARSER\\t注册扩展Model解析。\\n', 'PassBase\\tPass的基类。\\n', 'PassPosition\\t扩展Pass的运行位置。\\n', 'PassRegistry\\t扩展Pass注册构造类。\\n', 'REG_PASS\\t注册扩展Pass。\\n', 'REG_SCHEDULED_PASS\\t注册扩展Pass的调度顺序。\\n', 'RegisterKernel\\t算子注册实现类。\\n', 'KernelReg\\t算子注册构造类。\\n', 'REGISTER_KERNEL\\t注册算子。\\n', 'REGISTER_CUSTOM_KERNEL\\t注册Custom算子注册。\\n', 'RegisterKernelInterface\\t算子扩展能力注册实现类。\\n', 'KernelInterfaceReg\\t算子扩展能力注册构造类。\\n', 'REGISTER_KERNEL_INTERFACE\\t注册算子扩展能力。\\n', 'REGISTER_CUSTOM_KERNEL_INTERFACE\\t注册Custom算子扩展能力。\\n', 'NodeParserRegistry\\n', '#include <node_parser_registry.h>\\n', '\\n', 'NodeParserRegistry类用于注册及获取NodeParser类型的共享智能指针。\\n', '\\n', 'NodeParserRegistry\\n', 'NodeParserRegistry(converter::FmkType fmk_type, const std::string &node_type,\\n', '                   const converter::NodeParserPtr &node_parser);\\n', '构造函数。\\n', '\\n', '参数\\n', '\\n', 'fmk_type: 框架类型，具体见FmkType说明。\\n', '\\n', 'node_type: 节点的类型。\\n', '\\n', 'node_parser: NodeParser类型的共享智能指针实例, 具体见NodeParserPtr说明。\\n', '\\n', '~NodeParserRegistry\\n', '~NodeParserRegistry = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'GetNodeParser\\n', 'static converter::NodeParserPtr GetNodeParser(converter::FmkType fmk_type, const std::string &node_type);\\n', '静态方法，获取NodeParser类型的共享智能指针实例。\\n', '\\n', '参数\\n', '\\n', 'fmk_type: 框架类型，具体见FmkType说明。\\n', '\\n', 'node_type: 节点的类型。\\n', '\\n', 'REG_NODE_PARSER\\n', '#include <node_parser_registry.h>\\n', '\\n', '#define REG_NODE_PARSER(fmk_type, node_type, node_parser)\\n', '注册NodeParser宏。\\n', '\\n', '参数\\n', '\\n', 'fmk_type: 框架类型，具体见FmkType说明。\\n', '\\n', 'node_type: 节点的类型。\\n', '\\n', 'node_parser: NodeParser类型的共享智能指针实例, 具体见NodeParserPtr说明。\\n', '\\n', 'ModelParserCreator\\n', '#include <model_parser_registry.h>\\n', '\\n', 'typedef converter::ModelParser *(*ModelParserCreator)()\\n', '创建ModelParser的函数原型声明。\\n', '\\n', 'ModelParserRegistry\\n', '#include <model_parser_registry.h>\\n', '\\n', 'ModelParserRegistry类用于注册及获取ModelParserCreator类型的函数指针。\\n', '\\n', 'ModelParserRegistry\\n', 'ModelParserRegistry(FmkType fmk, ModelParserCreator creator)\\n', '构造函数，构造ModelParserRegistry对象，进行Model解析注册。\\n', '\\n', '参数\\n', '\\n', 'fmk: 框架类型，具体见FmkType说明。\\n', '\\n', 'creator: ModelParserCreator类型的函数指针, 具体见ModelParserCreator说明。\\n', '\\n', '~ModelParserRegistry\\n', '~ModelParserRegistry()\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'GetModelParser\\n', 'static ModelParser *GetModelParser(FmkType fmk)\\n', '获取ModelParserCreator类型的函数指针。\\n', '\\n', '参数\\n', '\\n', 'fmk: 框架类型，具体见FmkType说明。\\n', '\\n', 'REG_MODEL_PARSER\\n', '#include <model_parser_registry.h>\\n', '\\n', '#define REG_MODEL_PARSER(fmk, parserCreator)\\n', '注册ModelParserCreator类。\\n', '\\n', '参数\\n', '\\n', 'fmk: 框架类型，具体见FmkType说明。\\n', '\\n', 'creator: ModelParserCreator类型的函数指针, 具体见ModelParserCreator说明。\\n', '\\n', '用户自定义的ModelParser，框架类型必须满足设定支持的框架类型FmkType。\\n', '\\n', 'PassBase\\n', '#include <pass_base.h>\\n', '\\n', 'PassBase定义了图优化的基类，以供用户继承并自定义图优化算法。\\n', '\\n', 'PassBase\\n', 'PassBase(const std::string &name = \"PassBase\")\\n', '构造函数，构造PassBase类对象。\\n', '\\n', '参数\\n', '\\n', 'name: PassBase类对象的标识，需保证唯一性。\\n', '\\n', '~PassBase\\n', 'virtual ~PassBase() = default;\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'Execute\\n', 'virtual bool Execute(const api::FuncGraphPtr &func_graph) = 0;\\n', '对图进行操作的接口函数。\\n', '\\n', '参数\\n', '\\n', 'func_graph: FuncGraph的指针类对象。\\n', '\\n', 'PassBasePtr\\n', '#include <pass_base.h>\\n', '\\n', 'PassBase类的共享智能指针类型。\\n', '\\n', 'using PassBasePtr = std::shared_ptr<PassBase>\\n', 'PassPosition\\n', '#include <pass_registry.h>\\n', '\\n', 'enum类型变量，定义扩展Pass的运行位置。\\n', '\\n', 'enum PassPosition {\\n', '  POSITION_BEGIN = 0,    // 扩展Pass运行于内置融合Pass前\\n', '  POSITION_END = 1       // 扩展Pass运行于内置融合Pass后\\n', '};\\n', 'PassRegistry\\n', '#include <pass_registry.h>\\n', '\\n', 'PassRegistry类用于注册及获取Pass类实例。\\n', '\\n', 'PassRegistry\\n', 'PassRegistry(const std::string &pass_name, const PassBasePtr &pass)\\n', '构造函数，构造PassRegistry对象，进行注册Pass。\\n', '\\n', '参数\\n', '\\n', 'pass_name: Pass的命名标识，保证唯一性。\\n', '\\n', 'pass: PassBase类实例。\\n', '\\n', 'PassRegistry(PassPosition position, const std::vector<std::string> &names)\\n', '构造函数，构造PassRegistry对象，指定扩展Pass的运行位置及其运行顺序。\\n', '\\n', '参数\\n', '\\n', 'position: 扩展Pass的运行位置，具体见PassPosition说明。\\n', '\\n', 'names: 用户指定在该运行位置处，调用Pass的命名标识，命名标识的顺序即为指定Pass的调用顺序。\\n', '\\n', '~PassRegistry\\n', '~PassRegistry()\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'GetOuterScheduleTask\\n', 'static std::vector<std::string> GetOuterScheduleTask(PassPosition position)\\n', '获取指定位置处，外部设定的调度任务。\\n', '\\n', '参数\\n', '\\n', 'position: 扩展Pass的运行位置，具体见PassPosition说明。\\n', '\\n', 'GetPassFromStoreRoom\\n', 'static PassBasePtr GetPassFromStoreRoom(const std::string &pass_name)\\n', '获取PassBase实例，根据指定的Pass命名标识。\\n', '\\n', '参数\\n', '\\n', 'pass_name: Pass的命名标识。\\n', '\\n', 'REG_PASS\\n', '#include <pass_registry.h>\\n', '\\n', '#define REG_PASS(name, pass)\\n', '注册Pass宏。\\n', '\\n', '参数\\n', '\\n', 'name: Pass的命名标识，保证唯一性。\\n', '\\n', 'pass: PassBase类实例。\\n', '\\n', 'REG_SCHEDULED_PASS\\n', '#include <pass_registry.h>\\n', '\\n', '#define REG_SCHEDULED_PASS(position, names)\\n', '指定扩展Pass的运行位置及其运行顺序。\\n', '\\n', '参数\\n', '\\n', 'position: 扩展Pass的运行位置，具体见PassPosition说明。\\n', '\\n', 'names: 用户指定在该运行位置处，调用Pass的命名标识，命名标识的顺序即为指定Pass的调用顺序。\\n', '\\n', 'MindSpore Lite开放了部分内置Pass，请见以下说明。用户可以在names参数中添加内置Pass的命名标识，以在指定运行处调用内置Pass。\\n', '\\n', 'ConstFoldPass: 将输入均是常量的节点进行离线计算，导出的模型将不含该节点。特别地，针对shape算子，在inputShape给定的情形下，也会触发预计算。\\n', '\\n', 'DumpGraph: 导出当前状态下的模型。请确保当前模型为NHWC或者NCHW格式的模型，例如卷积算子等。\\n', '\\n', 'ToNCHWFormat: 将当前状态下的模型转换为NCHW的格式，例如，四维的图输入、卷积算子等。\\n', '\\n', 'ToNHWCFormat: 将当前状态下的模型转换为NHWC的格式，例如，四维的图输入、卷积算子等。\\n', '\\n', 'DecreaseTransposeAlgo: transpose算子的优化算法，删除冗余的transpose算子。\\n', '\\n', 'ToNCHWFormat与ToNHWCFormat需配套使用。在开放的运行位置处，用户所得到的模型已统一为NHWC的格式，用户也需确保在当前运行位置处返回之时，模型也是NHWC的格式。\\n', '\\n', '例: 指定names为{“ToNCHWFormat”， “UserPass”，”ToNHWCFormat”}。\\n', '\\n', 'KernelDesc\\n', '#include <registry/register_kernel.h>\\n', '\\n', 'struct类型结构体，定义扩展kernel的基本属性。\\n', '\\n', 'struct KernelDesc {\\n', '  DataType data_type;        // kernel的计算数据类型\\n', '  int type;                  // 算子的类型\\n', '  std::string arch;          // 设备标识\\n', '  std::string provider;      // 用户标识\\n', '};\\n', 'RegisterKernel\\n', '#include <registry/register_kernel.h>\\n', '\\n', 'CreateKernel\\n', 'using CreateKernel = std::function<std::shared_ptr<kernel::Kernel>(\\n', '  const std::vector<MSTensor> &inputs, const std::vector<MSTensor> &outputs, const schema::Primitive *primitive,\\n', '  const mindspore::Context *ctx)>\\n', '创建算子的函数原型声明。\\n', '\\n', '参数\\n', '\\n', 'inputs: 算子输入MSTensor。\\n', '\\n', 'outputs: 算子输出MSTensor。\\n', '\\n', 'primitive: 算子经由flatbuffers反序化为Primitive后的结果。\\n', '\\n', 'ctx: 算子的上下文Context。\\n', '\\n', '公有成员函数\\n', 'RegKernel\\n', 'static Status RegKernel(const std::string &arch, const std::string &provider, DataType data_type, int type, const CreateKernel creator)\\n', '算子注册。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'RegCustomKernel\\n', 'static Status RegCustomKernel(const std::string &arch, const std::string &provider, DataType data_type, const std::string &type, const CreateKernel creator)\\n', 'Custom算子注册。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'type: 算子类型，由用户自定义，确保唯一即可。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'GetCreator\\n', 'static CreateKernel GetCreator(const schema::Primitive *primitive, KernelDesc *desc);\\n', '获取算子的创建函数。\\n', '\\n', '参数\\n', '\\n', 'primitive: 算子经由flatbuffers反序化为Primitive后的结果。\\n', '\\n', 'desc: 算子的基本属性,具体见KernelDesc说明。\\n', '\\n', 'KernelReg\\n', '#include <registry/register_kernel.h>\\n', '\\n', '~KernelReg\\n', '~KernelReg() = default\\n', '析构函数。\\n', '\\n', 'KernelReg\\n', 'KernelReg(const std::string &arch, const std::string &provider, DataType data_type, int op_type, const CreateKernel creator)\\n', '构造函数，构造注册算子，进行算子注册。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'KernelReg(const std::string &arch, const std::string &provider, DataType data_type, const std::string &op_type, const CreateKernel creator)\\n', '构造函数，构造注册Custom算子，进行算子注册。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'op_type: 算子类型，由用户自定义，确保唯一即可。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'REGISTER_KERNEL\\n', '#define REGISTER_KERNEL(arch, provider, data_type, op_type, creator)\\n', '注册算子宏。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'REGISTER_CUSTOM_KERNEL\\n', '#define REGISTER_CUSTOM_KERNEL(arch, provider, data_type, op_type, creator)\\n', '注册Custom算子。\\n', '\\n', '参数\\n', '\\n', 'arch: 算子运行的平台，由用户自定义，如果算子是运行在CPU平台，或者算子运行完后的output tensor里的内存是在CPU平台上的，则此处也写CPU，MindSpore Lite内部会切成一个子图，在异构并行场景下有助于性能提升。\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'data_type: 算子支持的数据类型，具体见DataType。\\n', '\\n', 'op_type: 算子类型，由用户自定义，确保唯一即可。\\n', '\\n', 'creator: 创建算子的函数指针，具体见CreateKernel的说明。\\n', '\\n', 'KernelInterfaceCreator\\n', '#include <registry/register_kernel_interface.h>\\n', '\\n', '定义创建算子的函数指针类型。\\n', '\\n', 'using KernelInterfaceCreator = std::function<std::shared_ptr<kernel::KernelInterface>()>;\\n', 'RegisterKernelInterface\\n', '#include <registry/register_kernel_interface.h>\\n', '\\n', '算子扩展能力注册实现类。\\n', '\\n', '公有成员函数\\n', 'CustomReg\\n', 'static Status CustomReg(const std::string &provider, const std::string &op_type, const KernelInterfaceCreator creator)\\n', 'Custom算子的扩展能力注册。\\n', '\\n', '参数\\n', '\\n', 'provider: 生产商，由用户自定义。\\n', '\\n', 'op_type: 算子类型，由用户自定义。\\n', '\\n', 'creator: KernelInterface的创建函数，详细见KernelInterfaceCreator的说明。\\n', '\\n', 'Reg\\n', 'static Status Reg(const std::string &provider, int op_type, const KernelInterfaceCreator creator)\\n', '算子的扩展能力注册。\\n', '\\n', '参数\\n', '\\n', 'provider: 生产商，由用户自定义。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: KernelInterface的创建函数，详细见KernelInterfaceCreator的说明。\\n', '\\n', 'GetKernelInterface\\n', 'static std::shared_ptr<kernel::KernelInterface> GetKernelInterface(const std::string &provider, const schema::Primitive *primitive, const kernel::Kernel *kernel)\\n', '获取注册的算子扩展能力。\\n', '\\n', '参数\\n', '\\n', 'provider：生产商名，由用户自定义。\\n', '\\n', 'primitive：算子经过flatbuffers反序化后的结果，存储算子属性。\\n', '\\n', 'kernel：算子的内核，不传的话默认为空，为空时必须保证primitive非空有效。\\n', '\\n', 'KernelInterfaceReg\\n', '#include <registry/register_kernel_interface.h>\\n', '\\n', '算子扩展能力注册构造类。\\n', '\\n', 'KernelInterfaceReg\\n', 'KernelInterfaceReg(const std::string &provider, int op_type, const KernelInterfaceCreator creator)\\n', '构造函数，构造注册算子的扩展能力。\\n', '\\n', '参数\\n', '\\n', 'provider: 生产商，由用户自定义。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: KernelInterface的创建函数，详细见KernelInterfaceCreator的说明。\\n', '\\n', 'KernelInterfaceReg(const std::string &provider, const std::string &op_type, const KernelInterfaceCreator creator)\\n', '构造函数，构造注册custom算子的扩展能力。\\n', '\\n', '参数\\n', '\\n', 'provider: 生产商，由用户自定义。\\n', '\\n', 'op_type: 算子类型，由用户自定义。\\n', '\\n', 'creator: KernelInterface的创建函数，详细见KernelInterfaceCreator的说明。\\n', '\\n', 'REGISTER_KERNEL_INTERFACE\\n', '#include <registry/register_kernel_interface.h>\\n', '\\n', '注册KernelInterface的实现。\\n', '\\n', '#define REGISTER_KERNEL_INTERFACE(provider, op_type, creator)\\n', '参数\\n', '\\n', 'provider: 生产商，由用户自定义。\\n', '\\n', 'op_type: 算子类型，定义在ops.fbs中，编绎时会生成到ops_generated.h，该文件可以在发布件中获取。\\n', '\\n', 'creator: 创建KernelInterface的函数指针，具体见KernelInterfaceCreator的说明。\\n', '\\n', 'REGISTER_CUSTOM_KERNEL_INTERFACE\\n', '#include <registry/register_kernel_interface.h>\\n', '\\n', '注册Custom算子对应的KernelInterface实现。\\n', '\\n', '#define REGISTER_CUSTOM_KERNEL_INTERFACE(provider, op_type, creator)\\n', '参数\\n', '\\n', 'provider: 生产商名，由用户自定义。\\n', '\\n', 'op_type: 算子类型，由用户自定义，确保唯一同时要与REGISTER_CUSTOM_KERNEL时注册的op_type保持一致。\\n', '\\n', 'creator: 创建算子的函数指针，具体见KernelInterfaceCreator的说明。\\n']"}
{"index": {"_index": "r1.7-c++-api", "_id": "mindspore.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore.html", "text_entry": "['mindspore\\n', '\\n', '\\n', '接口汇总\\n', '类\\n', '类名\\t描述\\n', 'Context\\t保存执行中的环境变量。\\n', 'DeviceInfoContext\\t不同硬件设备的环境信息。\\n', 'CPUDeviceInfo\\t模型运行在CPU上的配置，仅MindSpore Lite支持。\\n', 'GPUDeviceInfo\\t模型运行在GPU上的配置。\\n', 'KirinNPUDeviceInfo\\t模型运行在NPU上的配置，仅MindSpore Lite支持。\\n', 'Ascend910DeviceInfo\\t模型运行在Ascend910上的配置，MindSpore Lite不支持。\\n', 'Ascend310DeviceInfo\\t模型运行在Ascend310上的配置。\\n', 'Serialization\\t汇总了模型文件读写的方法。\\n', 'Buffer\\tBuff数据类。\\n', 'Model\\tMindSpore中的模型，便于计算图管理。\\n', 'MSTensor\\tMindSpore中的张量。\\n', 'QuantParam\\tMSTensor中的一组量化参数。\\n', 'MSKernelCallBack\\tMindSpore回调函数包装器，仅MindSpore Lite支持。\\n', 'MSCallBackParam\\tMindSpore回调函数的参数，仅MindSpore Lite支持。\\n', 'Delegate\\tMindSpore Lite接入第三方AI框架的代理，仅MindSpore Lite支持。\\n', 'SchemaVersion\\tMindSpore Lite 执行推理时模型文件的版本，仅MindSpore Lite支持。\\n', 'KernelIter\\tMindSpore Lite 算子列表的迭代器，仅MindSpore Lite支持。\\n', 'DelegateModel\\tMindSpore Lite Delegate机制封装的模型，仅MindSpore Lite支持。\\n', 'TrainCfg\\tMindSpore Lite训练配置类，仅MindSpore Lite支持。\\n', 'MixPrecisionCfg\\tMindSpore Lite训练混合精度配置类，仅MindSpore Lite支持。\\n', 'AccuracyMetrics\\tMindSpore Lite训练精度类，仅MindSpore Lite支持。\\n', 'Metrics\\tMindSpore Lite训练指标类，仅MindSpore Lite支持。\\n', 'TrainCallBack\\tMindSpore Lite训练回调类，仅MindSpore Lite支持。\\n', 'TrainCallBackData\\t定义了训练回调的一组参数，仅MindSpore Lite支持。\\n', 'CkptSaver\\tMindSpore Lite训练模型文件保存类，仅MindSpore Lite支持。\\n', 'LossMonitor\\tMindSpore Lite训练学习率调度类，仅MindSpore Lite支持。\\n', 'LRScheduler\\tMindSpore Lite训练配置类，仅MindSpore Lite支持。\\n', 'StepLRLambda\\tMindSpore Lite训练学习率的一组参数，仅MindSpore Lite支持。\\n', 'MultiplicativeLRLambda\\t每个epoch将学习率乘以一个因子，仅MindSpore Lite支持。\\n', 'TimeMonitor\\tMindSpore Lite训练时间监测类，仅MindSpore Lite支持。\\n', 'TrainAccuracy\\tMindSpore Lite训练学习率调度类，仅MindSpore Lite支持。\\n', 'CharVersion\\t获取当前版本号，仅MindSpore Lite支持。\\n', 'Version\\t获取当前版本号，仅MindSpore Lite支持。\\n', 'Allocator\\t内存管理基类。\\n', 'Status\\t返回状态类。\\n', 'Graph\\t图类。\\n', 'CellBase\\t容器基类。\\n', 'Cell\\t容器类。\\n', 'GraphCell\\t图容器类。\\n', 'RunnerConfig\\t模型并发推理配置参数。\\n', 'ModelParallelRunner\\t模型并发推理类。\\n', '枚举\\n', '接口名\\t描述\\n', 'mindspore::DataType\\tMindSpore MSTensor保存的数据支持的类型。\\n', 'mindspore::Format\\tMindSpore MSTensor保存的数据支持的排列格式。\\n', '全局方法\\n', '方法名\\t描述\\n', 'StringToChar\\t将std::string转换成std::vector<char>。\\n', 'CharToString\\t将std::vector<char>转换成std::string。\\n', 'PairStringToChar\\t将std::pair<std::string, int32_t>转换成std::pair<std::vector<char>, int32_t>。\\n', 'PairCharToString\\t将std::pair<std::vector<char>, int32_t>转换成std::pair<std::string, int32_t>。\\n', 'VectorStringToChar\\t将std::vector<std::string>转换成std::vector<std::vector<char>>。\\n', 'VectorCharToString\\t将std::vector<std::vector<char>>转换成std::vector<std::string>。\\n', 'SetStringToChar\\t将std::set<std::string>转换成std::set<std::vector<char>>。\\n', 'SetCharToString\\t将std::set<std::vector<char>>转换成std::set<std::string>。\\n', 'MapStringToChar\\t将std::map<std::string, int32_t>转换成std::map<std::vector<char>, int32_t>。\\n', 'MapCharToString\\t将std::map<std::vector<char>, int32_t>转换成std::map<std::string, int32_t>。\\n', 'UnorderedMapStringToChar\\t将std::unordered_map<std::string, std::string>转换成std::map<std::vector<char>, std::vector<char>>。\\n', 'UnorderedMapCharToString\\t将std::map<std::vector<char>, std::vector<char>>转换成std::unordered_map<std::string, std::string>。\\n', 'ClassIndexStringToChar\\t将std::vector<std::pair<std::string, std::vector<int32_t>>>转换成std::vector<std::pair<std::vector<char>, std::vector<int32_t>>>。\\n', 'ClassIndexCharToString\\t将std::vector<std::pair<std::vector<char>, std::vector<int32_t>>>转换成std::vector<std::pair<std::string, std::vector<int32_t>>>。\\n', 'PairStringInt64ToPairCharInt64\\t将std::vector<std::pair<std::string, int64_t>>转换成std::vector<std::pair<std::vector<char>, int64_t>>。\\n', 'PadInfoStringToChar\\t将std::map<std::string, T>转换成std::map<std::vector<char>, T>。\\n', 'PadInfoCharToString\\t将std::map<std::vector<char>, T>转换成std::map<std::string, T>。\\n', 'TensorMapCharToString\\t将std::map<std::vector<char>, T>转换成std::unordered_map<std::string, T>。\\n', 'Context\\n', '#include <context.h>\\n', '\\n', 'Context类用于保存执行中的环境变量。\\n', '\\n', '构造函数和析构函数\\n', 'Context();\\n', '~Context() = default;\\n', '公有成员函数\\n', 'SetThreadNum\\n', 'void SetThreadNum(int32_t thread_num);\\n', '设置运行时的线程数，该选项仅MindSpore Lite有效。\\n', '\\n', '参数\\n', '\\n', 'thread_num: 运行时的线程数。\\n', '\\n', 'GetThreadNum\\n', 'int32_t GetThreadNum() const;\\n', '获取当前线程数设置。\\n', '\\n', '返回值\\n', '\\n', '当前线程数设置。\\n', '\\n', 'SetThreadAffinity\\n', 'void SetThreadAffinity(int mode);\\n', '设置运行时的CPU绑核策略，该选项仅MindSpore Lite有效。\\n', '\\n', '参数\\n', '\\n', 'mode: 绑核的模式，有效值为0-2，0为默认不绑核，1为绑大核，2为绑小核。\\n', '\\n', 'GetThreadAffinityMode\\n', 'int GetThreadAffinityMode() const;\\n', '获取当前CPU绑核策略，该选项仅MindSpore Lite有效。\\n', '\\n', '返回值\\n', '\\n', '当前CPU绑核策略，有效值为0-2，0为默认不绑核，1为绑大核，2为绑小核。\\n', '\\n', 'SetThreadAffinity\\n', 'void SetThreadAffinity(const std::vector<int> &core_list);\\n', '设置运行时的CPU绑核列表，该选项仅MindSpore Lite有效。如果SetThreadAffinity和SetThreadAffinity同时设置，core_list生效，mode不生效。\\n', '\\n', '参数\\n', '\\n', 'core_list: CPU绑核的列表。\\n', '\\n', 'GetThreadAffinityCoreList\\n', 'std::vector<int32_t> GetThreadAffinityCoreList() const;\\n', '获取当前CPU绑核列表，该选项仅MindSpore Lite有效。\\n', '\\n', '返回值\\n', '\\n', '当前CPU绑核列表。\\n', '\\n', 'SetEnableParallel\\n', 'void SetEnableParallel(bool is_parallel);\\n', '设置运行时是否支持并行，该选项仅MindSpore Lite有效。\\n', '\\n', '参数\\n', '\\n', 'is_parallel: bool量，为true则支持并行。\\n', '\\n', 'GetEnableParallel\\n', 'bool GetEnableParallel() const;\\n', '获取当前是否支持并行，该选项仅MindSpore Lite有效。\\n', '\\n', '返回值\\n', '\\n', '返回值为为true，代表支持并行。\\n', '\\n', 'SetDelegate\\n', 'void SetDelegate(const std::shared_ptr<Delegate> &delegate);\\n', '设置Delegate，Delegate定义了用于支持第三方AI框架接入的代理，该选项仅MindSpore Lite有效。\\n', '\\n', '参数\\n', '\\n', 'delegate: Delegate指针。\\n', '\\n', 'GetDelegate\\n', 'std::shared_ptr<Delegate> GetDelegate() const;\\n', '获取当前Delegate，该选项仅MindSpore Lite有效。\\n', '\\n', '返回值\\n', '\\n', '当前Delegate的指针。\\n', '\\n', 'MutableDeviceInfo\\n', 'std::vector<std::shared_ptr<DeviceInfoContext>> &MutableDeviceInfo();\\n', '修改该context下的DeviceInfoContext数组，仅MindSpore Lite支持数组中有多个成员是异构场景。\\n', '\\n', '返回值\\n', '\\n', '存储DeviceInfoContext的vector的引用。\\n', '\\n', 'DeviceInfoContext\\n', '#include <context.h>\\n', '\\n', 'DeviceInfoContext类定义不同硬件设备的环境信息。\\n', '\\n', '构造函数和析构函数\\n', 'DeviceInfoContext();\\n', 'virtual ~DeviceInfoContext() = default;\\n', '公有成员函数\\n', 'GetDeviceType\\n', 'virtual enum DeviceType GetDeviceType() const = 0;\\n', '获取该DeviceInfoContext的类型。\\n', '\\n', '返回值\\n', '\\n', '该DeviceInfoContext的类型。\\n', '\\n', 'enum DeviceType {\\n', '  kCPU = 0,\\n', '  kGPU,\\n', '  kKirinNPU,\\n', '  kAscend910,\\n', '  kAscend310,\\n', '  // add new type here\\n', '  kInvalidDeviceType = 100,\\n', '};\\n', 'Cast\\n', 'template <class T> std::shared_ptr<T> Cast();\\n', '在打开-fno-rtti编译选项的情况下提供类似RTTI的功能，将DeviceInfoContext转换为T类型的指针，若转换失败返回nullptr。\\n', '\\n', '返回值\\n', '\\n', '转换后T类型的指针，若转换失败则为nullptr。\\n', '\\n', 'GetProvider\\n', 'std::string GetProvider() const;\\n', '获取设备的生产商名。\\n', '\\n', 'SetProvider\\n', 'void SetProvider(const std::string &provider);\\n', '设置设备生产商名。\\n', '\\n', '参数\\n', '\\n', 'provider: 生产商名。\\n', '\\n', 'GetProviderDevice\\n', 'std::string GetProviderDevice() const;\\n', '获取生产商设备名。\\n', '\\n', 'SetProviderDevice\\n', 'void SetProviderDevice(const std::string &device);\\n', '设备生产商设备名。\\n', '\\n', '参数\\n', '\\n', 'device: 设备名。\\n', '\\n', 'SetAllocator\\n', 'void SetAllocator(const std::shared_ptr<Allocator> &allocator);\\n', '设置内存管理器。\\n', '\\n', '参数\\n', '\\n', 'allocator: 内存管理器。\\n', '\\n', 'GetAllocator\\n', 'std::shared_ptr<Allocator> GetAllocator() const;\\n', '获取内存管理器。\\n', '\\n', 'CPUDeviceInfo\\n', '#include <context.h>\\n', '\\n', '派生自DeviceInfoContext，模型运行在CPU上的配置，仅MindSpore Lite支持该选项。\\n', '\\n', '公有成员函数\\n', '函数\\t说明\\n', 'enum DeviceType GetDeviceType() const\\t- 返回值: DeviceType::kCPU\\n', 'void SetEnableFP16(bool is_fp16)\\t用于指定是否以FP16精度进行推理\\n', '\\n', '- is_fp16: 是否以FP16精度进行推理\\n', 'bool GetEnableFP16() const\\t- 返回值: 已配置的精度模式\\n', 'GPUDeviceInfo\\n', '#include <context.h>\\n', '\\n', '派生自DeviceInfoContext，模型运行在GPU上的配置，仅MindSpore Lite支持该选项。\\n', '\\n', '公有成员函数\\n', '函数\\t说明\\n', 'enum DeviceType GetDeviceType() const\\t- 返回值: DeviceType::kGPU\\n', 'void SetDeviceID(uint32_t device_id)\\t用于指定设备ID\\n', '\\n', '- device_id: 设备ID\\n', 'uint32_t GetDeviceID() const\\t- 返回值: 已配置的设备ID\\n', 'void SetPrecisionMode(const std::string &precision_mode)\\t用于指定推理时算子精度\\n', '\\n', '- precision_mode: 可选值origin(以模型中指定精度进行推理), fp16(以FP16精度进行推理)，默认值: origin\\n', 'std::string GetPrecisionMode() const\\t- 返回值: 已配置的精度模式\\n', 'int GetRankID() const\\t- 返回值: 当前运行的RANK ID\\n', 'void SetEnableFP16(bool is_fp16)\\t用于指定是否以FP16精度进行推理\\n', '\\n', '- is_fp16: 是否以FP16精度进行推理\\n', 'bool GetEnableFP16() const\\t- 返回值: 已配置的精度模式\\n', 'void SetGLContext(void *gl_context)\\t用于指定OpenGL EGLContext\\n', '\\n', '- *gl_context: 指向OpenGL EGLContext的指针\\n', 'void *GetGLContext() const\\t- 返回值: 已配置的指向OpenGL EGLContext的指针\\n', 'void SetGLDisplay(void *gl_display)\\t用于指定OpenGL EGLDisplay\\n', '\\n', '- *gl_display: 指向OpenGL EGLDisplay的指针\\n', 'void *GetGLDisplay() const\\t- 返回值: 已配置的指向OpenGL EGLDisplay的指针\\n', 'KirinNPUDeviceInfo\\n', '#include <context.h>\\n', '\\n', '派生自DeviceInfoContext，模型运行在NPU上的配置，仅MindSpore Lite支持该选项。\\n', '\\n', '公有成员函数\\n', '函数\\t说明\\n', 'enum DeviceType GetDeviceType() const\\t- 返回值: DeviceType::kGPU\\n', 'void SetFrequency(int frequency)\\t用于指定NPU频率\\n', '\\n', '- frequency: 设置为1（低功耗）、2（均衡）、3（高性能）、4（极致性能），默认为3\\n', 'int GetFrequency() const\\t- 返回值: 已配置的NPU频率模式\\n', 'Ascend910DeviceInfo\\n', '#include <context.h>\\n', '\\n', '派生自DeviceInfoContext，模型运行在Ascend910上的配置，MindSpore Lite不支持该选项。\\n', '\\n', '公有成员函数\\n', '函数\\t说明\\n', 'void SetDeviceID(uint32_t device_id)\\t用于指定设备ID\\n', '\\n', '- device_id: 设备ID\\n', 'uint32_t GetDeviceID() const\\t- 返回值: 已配置的设备ID\\n', 'Ascend310DeviceInfo\\n', '#include <context.h>\\n', '\\n', '派生自DeviceInfoContext，模型运行在Ascend310上的配置。\\n', '\\n', '公有成员函数\\n', '函数\\t说明\\n', 'void SetDeviceID(uint32_t device_id)\\t用于指定设备ID\\n', '\\n', '- device_id: 设备ID\\n', 'uint32_t GetDeviceID() const\\t- 返回值: 已配置的设备ID\\n', 'void SetInsertOpConfigPath(const std::string &cfg_path)\\t模型插入AIPP算子\\n', '\\n', '- cfg_path: AIPP配置文件路径\\n', 'std::string GetInsertOpConfigPath()\\t- 返回值: 已配置的AIPP\\n', 'void SetInputFormat(const std::string &format)\\t指定模型输入formatt\\n', '\\n', '- format: 可选有\"NCHW\"，\"NHWC\"等\\n', 'std::string GetInputFormat()\\t- 返回值: 已配置模型输入format\\n', 'void SetInputShape(const std::string &shape)\\t指定模型输入shape\\n', '\\n', '- shape: 如\"input_op_name1:1,2,3,4;input_op_name2:4,3,2,1\"\\n', 'std::string GetInputShape()\\t- 返回值: 已配置模型输入shape\\n', 'void SetOutputType(enum DataType output_type)\\t指定模型输出type\\n', '\\n', '- output_type: 仅支持uint8、fp16和fp32\\n', 'enum DataType GetOutputType()\\t- 返回值: 已配置模型输出type\\n', 'void SetPrecisionMode(const std::string &precision_mode)\\t配置模型精度模式\\n', '\\n', '- precision_mode: 可选有\"force_fp16\"，\"allow_fp32_to_fp16\"，\"must_keep_origin_dtype\"或者\"allow_mix_precision\"，默认为\"force_fp16\"\\n', 'std::string GetPrecisionMode(t)\\t- 返回值: 已配置模型精度模式\\n', 'void SetOpSelectImplMode(const std::string &op_select_impl_mode)\\t配置算子选择模式\\n', '\\n', '- op_select_impl_mode: 可选有\"high_performance\"和\"high_precision\"，默认为\"high_performance\"\\n', 'std::string GetOpSelectImplMode()\\t- 返回值: 已配置算子选择模式\\n', 'Serialization\\n', '#include <serialization.h>\\n', '\\n', 'Serialization类汇总了模型文件读写的方法。\\n', '\\n', '静态公有成员函数\\n', 'Load\\n', '从文件加载模型。\\n', '\\n', 'Status Load(const std::string &file, ModelType model_type, Graph *graph, const Key &dec_key = {},\\n', '            const std::string &dec_mode = kDecModeAesGcm);\\n', '参数\\n', '\\n', 'file: 模型文件路径。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Lite、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Lite类型。\\n', '\\n', 'graph: 输出参数，保存图数据的对象。\\n', '\\n', 'dec_key: 解密密钥，用于解密密文模型，密钥长度为16、24或32。\\n', '\\n', 'dec_mode: 解密模式，可选有AES-GCM、AES-CBC。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Load\\n', '从多个文件加载多个模型，MindSpore Lite未提供此功能。\\n', '\\n', 'Status Load(const std::vector<std::string> &files, ModelType model_type, std::vector<Graph> *graphs,\\n', '            const Key &dec_key = {}, const std::string &dec_mode = kDecModeAesGcm);\\n', '参数\\n', '\\n', 'files: 多个模型文件路径，用vector存储。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Lite、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Lite类型。\\n', '\\n', 'graphs: 输出参数，依次保存图数据的对象。\\n', '\\n', 'dec_key: 解密密钥，用于解密密文模型，密钥长度为16、24或32。\\n', '\\n', 'dec_mode: 解密模式，可选有AES-GCM、AES-CBC。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Load\\n', '从内存缓冲区加载模型。\\n', '\\n', 'Status Load(const void *model_data, size_t data_size, ModelType model_type, Graph *graph,\\n', '            const Key &dec_key = {}, const std::string &dec_mode = kDecModeAesGcm);\\n', '参数\\n', '\\n', 'model_data：模型数据指针。\\n', '\\n', 'data_size：模型数据字节数。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Lite、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Lite类型。\\n', '\\n', 'graph：输出参数，保存图数据的对象。\\n', '\\n', 'dec_key: 解密密钥，用于解密密文模型，密钥长度为16、24或32。\\n', '\\n', 'dec_mode: 解密模式，可选有AES-GCM、AES-CBC。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'SetParameters\\n', '配置模型参数。\\n', '\\n', 'static Status SetParameters(const std::map<std::string, Buffer> &parameters, Model *model);\\n', '参数\\n', '\\n', 'parameters：参数。\\n', '\\n', 'model：模型。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'ExportModel\\n', '导出训练模型，MindSpore Lite训练使用。\\n', '\\n', 'static Status ExportModel(const Model &model, ModelType model_type, Buffer *model_data);\\n', '参数\\n', '\\n', 'model：模型数据。\\n', '\\n', 'model_type：模型文件类型。\\n', '\\n', 'model_data：模型参数数据。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'ExportModel\\n', '导出训练模型，MindSpore Lite训练使用。\\n', '\\n', 'static Status ExportModel(const Model &model, ModelType model_type, const std::string &model_file,\\n', '                        QuantizationType quantization_type = kNoQuant, bool export_inference_only = true,\\n', '                        std::vector<std::string> output_tensor_name = {});\\n', '参数\\n', '\\n', 'model：模型数据。\\n', '\\n', 'model_type：模型文件类型。\\n', '\\n', 'model_file：保存的模型文件。\\n', '\\n', 'quantization_type: 量化类型。\\n', '\\n', 'export_inference_only: 是否导出只做推理的模型。\\n', '\\n', 'output_tensor_name: 设置导出的推理模型的输出张量的名称，默认为空，导出完整的推理模型。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Buffer\\n', '#include <types.h>\\n', '\\n', 'Buffer定义了MindSpore中Buffer数据的结构。\\n', '\\n', '构造函数和析构函数\\n', '  Buffer();\\n', '  Buffer(const void *data, size_t data_len);\\n', '  ~Buffer();\\n', '公有成员函数\\n', 'Data\\n', 'const void *Data() const;\\n', '获取只读的数据地址。\\n', '\\n', '返回值\\n', '\\n', 'const void指针。\\n', '\\n', 'MutableData\\n', 'void *MutableData();\\n', '获取可写的数据地址。\\n', '\\n', '返回值\\n', '\\n', 'void指针。\\n', '\\n', 'DataSize\\n', 'size_t DataSize() const;\\n', '获取data大小。\\n', '\\n', '返回值\\n', '\\n', '当前data大小。\\n', '\\n', 'ResizeData\\n', 'bool ResizeData(size_t data_len);\\n', '重置data大小。\\n', '\\n', '参数\\n', '\\n', 'data_len: data大小\\n', '\\n', '返回值\\n', '\\n', '是否配置成功。\\n', '\\n', 'SetData\\n', 'bool SetData(const void *data, size_t data_len);\\n', '配置Data和大小。\\n', '\\n', '参数\\n', '\\n', 'data: data地址\\n', '\\n', 'data_len: data大小\\n', '\\n', '返回值\\n', '\\n', '是否配置成功。\\n', '\\n', 'Clone\\n', 'Buffer Clone() const;\\n', '拷贝一份自身的副本。\\n', '\\n', '返回值\\n', '\\n', '指向副本的指针。\\n', '\\n', 'Model\\n', '#include <model.h>\\n', '\\n', 'Model定义了MindSpore中的模型，便于计算图管理。\\n', '\\n', '构造函数和析构函数\\n', 'Model();\\n', '~Model();\\n', '公有成员函数\\n', 'Build\\n', 'Status Build(GraphCell graph, const std::shared_ptr<Context> &model_context = nullptr,\\n', '             const std::shared_ptr<TrainCfg> &train_cfg = nullptr);\\n', '将GraphCell存储的模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'graph: GraphCell是Cell的一个派生，Cell目前没有开放使用。GraphCell可以由Graph构造，如model.Build(GraphCell(graph), context)。\\n', '\\n', 'model_context: 模型Context。\\n', '\\n', 'train_cfg: train配置文件TrainCfg。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build\\n', 'Status Build(const void *model_data, size_t data_size, ModelType model_type,\\n', '             const std::shared_ptr<Context> &model_context = nullptr);\\n', '从内存缓冲区加载模型，并将模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'model_data: 指向存储读入模型文件缓冲区的指针。\\n', '\\n', 'data_size: 缓冲区大小。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Opt、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Opt类型。\\n', '\\n', 'model_context: 模型Context。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build\\n', 'Status Build(const void *model_data, size_t data_size, ModelType model_type,\\n', '             const std::shared_ptr<Context> &model_context = nullptr, const Key &dec_key = {},\\n', '             const std::string &dec_mode = kDecModeAesGcm, const std::string &cropto_lib_path);\\n', '从内存缓冲区加载模型，并将模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'model_data: 指向存储读入模型文件缓冲区的指针。\\n', '\\n', 'data_size: 缓冲区大小。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Lite、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Lite类型。\\n', '\\n', 'model_context: 模型Context。\\n', '\\n', 'dec_key: 解密密钥，用于解密密文模型，密钥长度为16。\\n', '\\n', 'dec_mode: 解密模式，可选有AES-GCM。\\n', '\\n', 'cropto_lib_path: OpenSSL Crypto解密库路径。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build\\n', 'Status Build(const std::string &model_path, ModelType model_type,\\n', '             const std::shared_ptr<Context> &model_context = nullptr);\\n', '根据路径读取加载模型，并将模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'model_path: 模型文件路径。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Opt、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Opt类型。\\n', '\\n', 'model_context: 模型Context。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build\\n', 'Status Build(const std::string &model_path, ModelType model_type,\\n', '             const std::shared_ptr<Context> &model_context = nullptr, const Key &dec_key = {},\\n', '             const std::string &dec_mode = kDecModeAesGcm, const std::string &cropto_lib_path);\\n', '根据路径读取加载模型，并将模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'model_path: 模型文件路径。\\n', '\\n', 'model_type: 模型文件类型，可选有ModelType::kMindIR、ModelType::kMindIR_Lite、ModelType::kOM。MindSpore Lite支持ModelType::kMindIR、ModelType::kMindIR_Lite类型。\\n', '\\n', 'model_context: 模型Context。\\n', '\\n', 'dec_key: 解密密钥，用于解密密文模型，密钥长度为16。\\n', '\\n', 'dec_mode: 解密模式，可选有AES-GCM。\\n', '\\n', 'cropto_lib_path: OpenSSL Crypto解密库路径。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build之后对model_context的其他修改不再生效。\\n', '\\n', 'Predict\\n', 'Status Predict(const std::vector<MSTensor> &inputs, std::vector<MSTensor> *outputs, const MSKernelCallBack &before = nullptr, const MSKernelCallBack &after = nullptr)\\n', '推理模型。\\n', '\\n', '参数\\n', '\\n', 'inputs: 模型输入按顺序排列的vector。\\n', '\\n', 'outputs: 输出参数，按顺序排列的vector的指针，模型输出会按顺序填入该容器。\\n', '\\n', 'before: 一个MSKernelCallBack 结构体。定义了运行每个节点之前调用的回调函数。\\n', '\\n', 'after: 一个MSKernelCallBack 结构体。定义了运行每个节点之后调用的回调函数。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'LoadConfig\\n', 'Status LoadConfig(const std::string &config_path);\\n', '根据路径读取配置文件。\\n', '\\n', '参数\\n', '\\n', 'config_path: 配置文件路径。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', '用户可以调用LoadConfig接口进行混合精度推理的设置，配置文件举例如下：\\n', '\\n', '[execution_plan]\\n', '\\n', 'op_name1=data_type:float16\\n', '\\n', 'op_name2=data_type:float32\\n', '\\n', 'UpdateConfig\\n', 'Status UpdateConfig(const std::string &section, const std::pair<std::string, std::string> &config);\\n', '刷新配置，读文件相对比较费时，如果少部分配置发生变化可以通过该接口更新部分配置。\\n', '\\n', '参数\\n', '\\n', 'section: 配置的章节名。\\n', '\\n', 'config: 要更新的配置对。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetInputs\\n', 'std::vector<MSTensor> GetInputs();\\n', '获取模型所有输入张量。\\n', '\\n', '返回值\\n', '\\n', '包含模型所有输入张量的容器类型变量。\\n', '\\n', 'GetInputByTensorName\\n', 'MSTensor GetInputByTensorName(const std::string &tensor_name);\\n', '获取模型指定名字的输入张量。\\n', '\\n', '返回值\\n', '\\n', '指定名字的输入张量，如果该名字不存在则返回非法张量。\\n', '\\n', 'GetGradients\\n', 'std::vector<MSTensor> GetGradients() const;\\n', '获取所有Tensor的梯度。\\n', '\\n', '返回值\\n', '\\n', '获取所有Tensor的梯度。\\n', '\\n', 'ApplyGradients\\n', 'Status ApplyGradients(const std::vector<MSTensor> &gradients);\\n', '应用所有Tensor的梯度。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetOptimizerParams\\n', 'std::vector<MSTensor> GetOptimizerParams() const;\\n', '获取optimizer参数MSTensor。\\n', '\\n', '返回值\\n', '\\n', '所有optimizer参数MSTensor。\\n', '\\n', 'SetOptimizerParams\\n', 'Status SetOptimizerParams(const std::vector<MSTensor> &params);\\n', '更新optimizer参数。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetOutputs\\n', 'std::vector<MSTensor> GetOutputs();\\n', '获取模型所有输出张量。\\n', '\\n', '返回值\\n', '\\n', '包含模型所有输出张量的容器类型变量。\\n', '\\n', 'GetOutputTensorNames\\n', 'std::vector<std::string> GetOutputTensorNames();\\n', '获取模型所有输出张量的名字。\\n', '\\n', '返回值\\n', '\\n', '包含模型所有输出张量名字的容器类型变量。\\n', '\\n', 'GetOutputByTensorName\\n', 'MSTensor GetOutputByTensorName(const std::string &tensor_name);\\n', '获取模型指定名字的输出张量。\\n', '\\n', '返回值\\n', '\\n', '指定名字的输出张量，如果该名字不存在则返回非法张量。\\n', '\\n', 'GetOutputsByNodeName\\n', 'std::vector<MSTensor> GetOutputsByNodeName(const std::string &node_name);\\n', '通过节点名获取模型的MSTensors输出张量。不建议使用，将在2.0版本废弃。\\n', '\\n', '参数\\n', '\\n', 'node_name: 节点名称。\\n', '\\n', '返回值\\n', '\\n', '包含在模型输出Tensor中的该节点输出Tensor的vector。\\n', '\\n', 'BindGLTexture2DMemory\\n', '  Status BindGLTexture2DMemory(const std::map<std::string, unsigned int> &inputGLTexture,\\n', '                               std::map<std::string, unsigned int> *outputGLTexture);\\n', '将OpenGL纹理数据与模型的输入和输出进行绑定。\\n', '\\n', '参数\\n', '\\n', 'inputGLTexture: 模型输入的OpenGL纹理数据, key为输入Tensor的名称，value为OpenGL纹理。\\n', '\\n', 'outputGLTexture: 模型输出的OpenGL纹理数据，key为输出Tensor的名称，value为OpenGL纹理。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'InitMetrics\\n', 'Status InitMetrics(std::vector<Metrics *> metrics);\\n', '训练指标参数初始化。\\n', '\\n', '参数\\n', '\\n', 'metrics: 训练指标参数。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetMetrics\\n', 'std::vector<Metrics *> GetMetrics();\\n', '获取训练指标参数。\\n', '\\n', '返回值\\n', '\\n', '训练指标参数。\\n', '\\n', 'SetTrainMode\\n', 'Status SetTrainMode(bool train);\\n', 'session设置训练模式。\\n', '\\n', '参数\\n', '\\n', 'train: 是否为训练模式。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetTrainMode\\n', 'bool GetTrainMode() const;\\n', '获取session是否是训练模式。\\n', '\\n', '返回值\\n', '\\n', 'bool类型，表示是否是训练模式。\\n', '\\n', 'Train\\n', 'Status Train(int epochs, std::shared_ptr<dataset::Dataset> ds, std::vector<TrainCallBack *> cbs);\\n', '模型训练。\\n', '\\n', '参数\\n', '\\n', 'epochs: 迭代轮数。\\n', '\\n', 'ds: 训练数据。\\n', '\\n', 'cbs: 包含训练回调类对象的vector。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Evaluate\\n', 'Status Evaluate(std::shared_ptr<dataset::Dataset> ds, std::vector<TrainCallBack *> cbs);\\n', '模型验证。\\n', '\\n', '参数\\n', '\\n', 'ds: 训练数据。\\n', '\\n', 'cbs: 包含训练回调类对象的vector。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Resize\\n', 'Status Resize(const std::vector<MSTensor> &inputs, const std::vector<std::vector<int64_t>> &dims);\\n', '调整已编译模型的输入形状。\\n', '\\n', '参数\\n', '\\n', 'inputs: 模型输入按顺序排列的vector。\\n', '\\n', 'dims: 输入形状，按输入顺序排列的由形状组成的vector，模型会按顺序依次调整张量形状。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'CheckModelSupport\\n', 'static bool CheckModelSupport(enum DeviceType device_type, ModelType model_type);\\n', '检查设备是否支持该模型。\\n', '\\n', '参数\\n', '\\n', 'device_type: 设备类型，例如kMaliGPU。\\n', '\\n', 'model_type: 模型类型，例如MindIR。\\n', '\\n', '返回值\\n', '\\n', '状态码。\\n', '\\n', 'MSTensor\\n', '#include <types.h>\\n', '\\n', 'MSTensor定义了MindSpore中的张量。\\n', '\\n', '构造函数和析构函数\\n', 'MSTensor();\\n', 'explicit MSTensor(const std::shared_ptr<Impl> &impl);\\n', 'MSTensor(const std::string &name, DataType type, const std::vector<int64_t> &shape, const void *data, size_t data_len);\\n', 'explicit MSTensor(std::nullptr_t);\\n', '~MSTensor();\\n', '注意：MSTensor构造时，若data指针通过malloc生成，用户在构造完成MSTensor后，需自行释放free，否则存在内存泄露。\\n', '\\n', '静态公有成员函数\\n', 'CreateTensor\\n', 'MSTensor *CreateTensor(const std::string &name, DataType type, const std::vector<int64_t> &shape,\\n', '                       const void *data, size_t data_len) noexcept;\\n', '创建一个MSTensor对象，其数据需复制后才能由Model访问，必须与DestroyTensorPtr成对使用。\\n', '\\n', '参数\\n', '\\n', 'name: 名称。\\n', '\\n', 'type：数据类型。\\n', '\\n', 'shape：形状。\\n', '\\n', 'data：数据指针，指向一段已开辟的内存。\\n', '\\n', 'data_len：数据长度，以字节为单位。\\n', '\\n', '返回值\\n', '\\n', 'MStensor指针。\\n', '\\n', 'CreateRefTensor\\n', 'MSTensor *CreateRefTensor(const std::string &name, DataType type, const std::vector<int64_t> &shape, void *data,\\n', '                          size_t data_len) noexcept;\\n', '创建一个MSTensor对象，其数据可以直接由Model访问，必须与DestroyTensorPtr成对使用。\\n', '\\n', '参数\\n', '\\n', 'name: 名称。\\n', '\\n', 'type：数据类型。\\n', '\\n', 'shape：形状。\\n', '\\n', 'data：数据指针，指向一段已开辟的内存。\\n', '\\n', 'data_len：数据长度，以字节为单位。\\n', '\\n', '返回值\\n', '\\n', 'MStensor指针。\\n', '\\n', 'CreateDevTensor\\n', 'static inline MSTensor *CreateDevTensor(const std::string &name, DataType type, const std::vector<int64_t> &shape,\\n', '                                        const void *data, size_t data_len) noexcept;\\n', '创建一个MSTensor对象，其device数据可以直接由Model访问，必须与DestroyTensorPtr成对使用。\\n', '\\n', '参数\\n', '\\n', 'name: 名称。\\n', '\\n', 'type：数据类型。\\n', '\\n', 'shape：形状。\\n', '\\n', 'data：数据指针，指向一段已开辟的device内存。\\n', '\\n', 'data_len：数据长度，以字节为单位。\\n', '\\n', '返回值\\n', '\\n', 'MStensor指针。\\n', '\\n', 'StringsToTensor\\n', 'MSTensor *StringsToTensor(const std::string &name, const std::vector<std::string> &str);\\n', '创建一个字符串类型的MSTensor对象，其数据需复制后才能由Model访问，必须与DestroyTensorPtr成对使用。\\n', '\\n', '参数\\n', '\\n', 'name: 名称。\\n', '\\n', 'str：装有若干个字符串的vector容器。\\n', '\\n', '返回值\\n', '\\n', 'MStensor指针。\\n', '\\n', 'TensorToStrings\\n', 'std::vector<std::string> TensorToStrings(const MSTensor &tensor);\\n', '将字符串类型的MSTensor对象解析为字符串。\\n', '\\n', '参数\\n', '\\n', 'tensor: 张量对象。\\n', '\\n', '返回值\\n', '\\n', '装有若干个字符串的vector容器。\\n', '\\n', 'DestroyTensorPtr\\n', 'void DestroyTensorPtr(MSTensor *tensor) noexcept;\\n', '销毁一个由Clone、StringsToTensor、CreateRefTensor或CreateTensor所创建的对象，请勿用于销毁其他来源的MSTensor。\\n', '\\n', '参数\\n', '\\n', 'tensor: 由Clone、StringsToTensor、CreateRefTensor或CreateTensor返回的指针。\\n', '\\n', '公有成员函数\\n', 'Name\\n', 'std::string Name() const;\\n', '获取MSTensor的名字。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的名字。\\n', '\\n', 'DataType\\n', 'enum DataType DataType() const;\\n', '获取MSTensor的数据类型。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的数据类型。\\n', '\\n', 'Shape\\n', 'const std::vector<int64_t> &Shape() const;\\n', '获取MSTensor的Shape。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的Shape。\\n', '\\n', 'ElementNum\\n', 'int64_t ElementNum() const;\\n', '获取MSTensor的元素个数。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的元素个数。\\n', '\\n', 'Data\\n', 'std::shared_ptr<const void> Data() const;\\n', '获取指向MSTensor中的数据拷贝的智能指针。\\n', '\\n', '返回值\\n', '\\n', '指向MSTensor中的数据拷贝的智能指针。\\n', '\\n', 'MutableData\\n', 'void *MutableData();\\n', '获取MSTensor中的数据的指针。如果为空指针，为MSTensor的数据申请内存，并返回申请内存的地址，如果不为空，返回数据的指针。\\n', '\\n', '返回值\\n', '\\n', '指向MSTensor中的数据的指针。\\n', '\\n', 'DataSize\\n', 'size_t DataSize() const;\\n', '获取MSTensor中的数据的以字节为单位的内存长度。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor中的数据的以字节为单位的内存长度。\\n', '\\n', 'IsDevice\\n', 'bool IsDevice() const;\\n', '判断MSTensor中是否在设备上。仅MindSpore云侧支持。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor中是否在设备上。\\n', '\\n', 'Clone\\n', 'MSTensor *Clone() const;\\n', '拷贝一份自身的副本。\\n', '\\n', '返回值\\n', '\\n', '指向深拷贝副本的指针，必须与DestroyTensorPtr成对使用。\\n', '\\n', 'operator==(std::nullptr_t)\\n', 'bool operator==(std::nullptr_t) const;\\n', '判断MSTensor是否合法。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor是否合法。\\n', '\\n', 'operator!=(std::nullptr_t)\\n', 'bool operator!=(std::nullptr_t) const;\\n', '判断MSTensor是否合法。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor是否合法。\\n', '\\n', 'operator==(const MSTensor &tensor)\\n', 'bool operator==(const MSTensor &tensor) const;\\n', '判断MSTensor是否与另一个MSTensor相等，仅MindSpore Lite支持。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor是否与另一个MSTensor相等。\\n', '\\n', 'SetShape\\n', 'void SetShape(const std::vector<int64_t> &shape);\\n', '设置MSTensor的Shape，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetDataType\\n', 'void SetDataType(enum DataType data_type);\\n', '设置MSTensor的DataType，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetTensorName\\n', 'void SetTensorName(const std::string &name);\\n', '设置MSTensor的名字，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetAllocator\\n', 'void SetAllocator(std::shared_ptr<Allocator> allocator);\\n', '设置MSTensor数据所属的内存池，仅MindSpore Lite支持。\\n', '\\n', '参数\\n', '\\n', 'model: 指向Allocator的指针。\\n', '\\n', 'allocator\\n', 'std::shared_ptr<Allocator> allocator() const;\\n', '获取MSTensor数据所属的内存池，仅MindSpore Lite支持。\\n', '\\n', '返回值\\n', '\\n', '指向Allocator的指针。\\n', '\\n', 'SetFormat\\n', 'void SetFormat(mindspore::Format format);\\n', '设置MSTensor数据的format，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'format\\n', 'mindspore::Format format() const;\\n', '获取MSTensor数据的format，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetData\\n', 'void SetData(void *data);\\n', '设置指向MSTensor数据的指针。\\n', '\\n', 'QuantParams\\n', 'std::vector<QuantParam> QuantParams() const;\\n', '获取MSTensor的量化参数，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetQuantParams\\n', 'void SetQuantParams(std::vector<QuantParam> quant_params);\\n', '设置MSTensor的量化参数，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'PadInfo\\n', 'const std::vector<std::vector<int64_t>> &PadInfo() const;\\n', '获取MSTensor的Pad信息，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetPadInfo\\n', 'void SetPadInfo(const std::vector<std::vector<int64_t>> &pad_info);\\n', '设置MSTensor的Pad信息，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'RawDataShape\\n', 'const std::vector<int64_t> &RawDataShape() const;\\n', '获取MSTensor的RawDataShape，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'SetRawDataShape\\n', 'void SetRawDataShape(const std::vector<int64_t> &raw_data_shape);\\n', '设置MSTensor的RawDataShape，仅MindSpore Lite支持，目前在Delegate机制使用。\\n', '\\n', 'impl\\n', 'const std::shared_ptr<Impl> impl()\\n', '获取实现类的指针，仅MindSpore Lite支持。\\n', '\\n', 'QuantParam\\n', '#include <types.h>\\n', '\\n', '一个结构体。QuantParam定义了MSTensor的一组量化参数。\\n', '\\n', '公有属性\\n', 'bit_num\\n', 'bit_num\\n', 'int 类型变量。量化的bit数。\\n', '\\n', 'scale\\n', 'scale\\n', 'double 类型变量。\\n', '\\n', 'zero_point\\n', 'zero_point\\n', 'int32_t 类型变量。\\n', '\\n', 'MSKernelCallBack\\n', '#include <types.h>\\n', '\\n', 'using MSKernelCallBack = std::function<bool(const std::vector<MSTensor> &inputs, const std::vector<MSTensor> &outputs, const MSCallBackParam &opInfo)>\\n', '一个函数包装器。MSKernelCallBack 定义了指向回调函数的指针。\\n', '\\n', 'MSCallBackParam\\n', '#include <types.h>\\n', '\\n', '一个结构体。MSCallBackParam定义了回调函数的输入参数。\\n', '\\n', '公有属性\\n', 'node_name\\n', 'node_name\\n', 'string 类型变量。节点名参数。\\n', '\\n', 'node_type\\n', 'node_type\\n', 'string 类型变量。节点类型参数。\\n', '\\n', 'Delegate\\n', '#include <delegate.h>\\n', '\\n', 'Delegate定义了第三方AI框架接入MindSpore Lite的代理接口。\\n', '\\n', '构造函数和析构函数\\n', 'Delegate() = default;\\n', 'virtual ~Delegate() = default;\\n', '公有成员函数\\n', 'Init\\n', 'virtual Status Init() = 0;\\n', '初始化Delegate资源。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Build\\n', 'virtual Status Build(DelegateModel *model) = 0;\\n', 'Delegate在线构图。\\n', '\\n', '参数\\n', '\\n', 'model: 指向存储DelegateModel实例的指针。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'SchemaVersion\\n', '#include <delegate.h>\\n', '\\n', '定义了Lite执行在线推理时模型文件的版本。\\n', '\\n', 'typedef enum {\\n', '  SCHEMA_INVALID = -1, /**< invalid version */\\n', '  SCHEMA_CUR,          /**< current version for ms model defined in model.fbs*/\\n', '  SCHEMA_V0,           /**< previous version for ms model defined in model_v0.fbs*/\\n', '} SchemaVersion;\\n', 'KernelIter\\n', '#include <delegate.h>\\n', '\\n', '定义了Lite Kernel列表的迭代器。\\n', '\\n', 'using KernelIter = std::vector<kernel::Kernel *>::iterator;\\n', 'DelegateModel\\n', '#include <delegate.h>\\n', '\\n', 'DelegateModel定义了MindSpore Lite Delegate机制操作的的模型对象。\\n', '\\n', '构造函数\\n', 'DelegateModel(std::vector<kernel::Kernel *> *kernels, const std::vector<MSTensor> &inputs,\\n', '              const std::vector<MSTensor> &outputs,\\n', '              const std::map<kernel::Kernel *, const schema::Primitive *> &primitives, SchemaVersion version);\\n', '析构函数\\n', '~DelegateModel() = default;\\n', '保护成员\\n', 'kernels_\\n', 'std::vector<kernel::Kernel *> *kernels_;\\n', 'Kernel的列表，保存模型的所有算子。\\n', '\\n', 'inputs_\\n', 'const std::vector<mindspore::MSTensor> &inputs_;\\n', 'MSTensor的列表，保存这个算子的输入tensor。\\n', '\\n', 'outputs_\\n', 'const std::vector<mindspore::MSTensor> &outputs;\\n', 'MSTensor的列表，保存这个算子的输出tensor。\\n', '\\n', 'primitives_\\n', 'const std::map<kernel::Kernel *, const schema::Primitive *> &primitives_;\\n', 'Kernel和schema::Primitive的Map，保存所有算子的属性。\\n', '\\n', 'version_\\n', 'SchemaVersion version_;\\n', 'enum值，当前执行推理的模型的版本SchemaVersion。\\n', '\\n', '公有成员函数\\n', 'GetPrimitive\\n', 'const schema::Primitive *GetPrimitive(kernel::Kernel *kernel) const;\\n', '获取一个Kernel的属性值。\\n', '\\n', '参数\\n', '\\n', 'kernel: 指向Kernel的指针。\\n', '\\n', '返回值\\n', '\\n', 'const schema::Primitive *，输入参数Kernel对应的该算子的属性值。\\n', '\\n', 'BeginKernelIterator\\n', 'KernelIter BeginKernelIterator();\\n', '返回DelegateModel Kernel列表起始元素的迭代器。\\n', '\\n', '返回值\\n', '\\n', 'KernelIter，指向DelegateModel Kernel列表起始元素的迭代器。\\n', '\\n', 'EndKernelIterator\\n', 'KernelIter EndKernelIterator();\\n', '返回DelegateModel Kernel列表末尾元素的迭代器。\\n', '\\n', '返回值\\n', '\\n', 'KernelIter，指向DelegateModel Kernel列表末尾元素的迭代器。\\n', '\\n', 'Replace\\n', 'KernelIter Replace(KernelIter from, KernelIter end, kernel::Kernel *graph_kernel);\\n', '用Delegate子图Kernel替换Delegate支持的连续Kernel列表。\\n', '\\n', '参数\\n', '\\n', 'from: Delegate支持的连续Kernel列表的起始元素迭代器。\\n', '\\n', 'end: Delegate支持的连续Kernel列表的末尾元素迭代器。\\n', '\\n', 'graph_kernel: 指向Delegate子图Kernel实例的指针。\\n', '\\n', '返回值\\n', '\\n', 'KernelIter，用Delegate子图Kernel替换之后，子图Kernel下一个元素的迭代器，指向下一个未被访问的Kernel。\\n', '\\n', 'inputs\\n', 'const std::vector<mindspore::MSTensor> &inputs();\\n', '返回DelegateModel输入tensor列表。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的列表。\\n', '\\n', 'outputs\\n', 'const std::vector<mindspore::MSTensor> &outputs();\\n', '返回DelegateModel输出tensor列表。\\n', '\\n', '返回值\\n', '\\n', 'MSTensor的列表。\\n', '\\n', 'GetVersion\\n', 'const SchemaVersion GetVersion() { return version_; }\\n', '返回当前执行推理的模型文件的版本。\\n', '\\n', '返回值\\n', '\\n', 'enum值，0: r1.2及r1.2之后的版本，1: r1.1及r1.1之前的版本，-1: 无效版本。\\n', '\\n', 'TrainCfg\\n', '#include <cfg.h>\\n', '\\n', 'TrainCfgMindSpore Lite训练的相关配置参数。\\n', '\\n', '构造函数\\n', 'TrainCfg() { this->loss_name_ = \"_loss_fn\"; }\\n', '公有成员变量\\n', 'OptimizationLevel optimization_level_ = kO0;\\n', '优化的数据类型。\\n', '\\n', 'enum OptimizationLevel : uint32_t {\\n', '  kO0 = 0,\\n', '  kO2 = 2,\\n', '  kO3 = 3,\\n', '  kAuto = 4,\\n', '  kOptimizationType = 0xFFFFFFFF\\n', '};\\n', 'std::string loss_name_;\\n', '损失节点的名称。\\n', '\\n', 'MixPrecisionCfg mix_precision_cfg_;\\n', '混合精度配置。\\n', '\\n', 'bool accumulate_gradients_;\\n', '是否累积梯度。\\n', '\\n', 'MixPrecisionCfg\\n', '#include <cfg.h>\\n', '\\n', 'MixPrecisionCfgMindSpore Lite训练混合精度配置类。\\n', '\\n', '构造函数\\n', '  MixPrecisionCfg() {\\n', '    dynamic_loss_scale_ = false;\\n', '    loss_scale_ = 128.0f;\\n', '    num_of_not_nan_iter_th_ = 1000;\\n', '  }\\n', '共有成员变量\\n', 'bool dynamic_loss_scale_ = false;\\n', '混合精度训练中是否启用动态损失比例。\\n', '\\n', 'float loss_scale_;\\n', '初始损失比例。\\n', '\\n', 'uint32_t num_of_not_nan_iter_th_;\\n', '动态损失阈值。\\n', '\\n', 'bool is_raw_mix_precision_;\\n', '原始模型是否是原生混合精度模型。\\n', '\\n', 'AccuracyMetrics\\n', '#include <accuracy.h>\\n', '\\n', 'AccuracyMetricsMindSpore Lite训练精度类。\\n', '\\n', '构造函数和析构函数\\n', 'explicit AccuracyMetrics(int accuracy_metrics = METRICS_CLASSIFICATION, const std::vector<int> &input_indexes = {1}, const std::vector<int> &output_indexes = {0});\\n', 'virtual ~AccuracyMetrics();\\n', '公有成员函数\\n', 'Clear\\n', 'void Clear() override;\\n', '精度清零。\\n', '\\n', 'Eval\\n', 'float Eval() override;\\n', '模型验证。\\n', '\\n', '返回值\\n', '\\n', 'float，模型验证精度。\\n', '\\n', 'Metrics\\n', '#include <metrics.h>\\n', '\\n', 'MetricsMindSpore Lite训练指标类。\\n', '\\n', '析构函数\\n', 'virtual ~Metrics() = default;\\n', '公有成员函数\\n', 'Clear\\n', 'virtual void Clear() {}\\n', '训练指标清零。\\n', '\\n', 'Eval\\n', 'virtual float Eval() { return 0.0; }\\n', '模型验证。\\n', '\\n', '返回值\\n', '\\n', 'float，模型验证精度。\\n', '\\n', 'Update\\n', 'virtual void Update(std::vector<MSTensor *> inputs, std::vector<MSTensor *> outputs) {}\\n', '模型输入输出数据更新。\\n', '\\n', '参数\\n', '\\n', 'inputs: 模型输入MSTensor的vector。\\n', '\\n', 'outputs: 模型输输出MSTensor的vector。\\n', '\\n', 'TrainCallBack\\n', '#include <callback.h>\\n', '\\n', 'MetricsMindSpore Lite训练回调类。\\n', '\\n', '析构函数\\n', 'virtual ~TrainCallBack() = default;\\n', '公有成员函数\\n', 'Begin\\n', 'virtual void Begin(const TrainCallBackData &cb_data) {}\\n', '网络执行前调用。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', 'End\\n', '  virtual void End(const TrainCallBackData &cb_data) {}\\n', '网络执行后调用。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', 'EpochBegin\\n', '  virtual void EpochBegin(const TrainCallBackData &cb_data) {}\\n', '每轮迭代前回调。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', 'EpochEnd\\n', '  virtual CallbackRetValue EpochEnd(const TrainCallBackData &cb_data) { return kContinue; }\\n', '每轮迭代后回调。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', '返回值\\n', '\\n', 'CallbackRetValue，表示是否在训练中继续循环。\\n', '\\n', 'enum CallbackRetValue : uint32_t {\\n', '  kContinue = 0,\\n', '  kStopTraining = 1,\\n', '  kExit = 2,\\n', '  kUnknownRetValue = 0xFFFFFFFF\\n', '};\\n', 'StepBegin\\n', '  virtual void StepBegin(const TrainCallBackData &cb_data) {}\\n', '每步迭代前回调。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', 'StepEnd\\n', '  virtual void StepEnd(const TrainCallBackData &cb_data) {}\\n', '每步迭代后回调。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', 'TrainCallBackData\\n', '#include <callback.h>\\n', '\\n', '一个结构体。TrainCallBackData定义了训练回调的一组参数。\\n', '\\n', '公有属性\\n', 'train_mode_\\n', 'train_mode_\\n', 'bool 类型变量。训练模式。\\n', '\\n', 'epoch_\\n', 'epoch_\\n', 'unsigned int 类型变量。训练迭代的epoch次数。\\n', '\\n', 'step_\\n', 'step_\\n', 'unsigned int 类型变量。训练迭代的step次数。\\n', '\\n', 'model_\\n', 'model_\\n', 'Model 类型指针。训练模型对象。\\n', '\\n', 'CkptSaver\\n', '#include <ckpt_saver.h>\\n', '\\n', 'MetricsMindSpore Lite训练模型文件保存类。\\n', '\\n', '构造函数和析构函数\\n', '  explicit CkptSaver(int save_every_n, const std::string &filename_prefix);\\n', '  virtual ~CkptSaver();\\n', 'LossMonitor\\n', '#include <loss_monitor.h>\\n', '\\n', 'MetricsMindSpore Lite训练损失函数类。\\n', '\\n', '构造函数和析构函数\\n', '  explicit LossMonitor(int print_every_n_steps = INT_MAX);\\n', '  virtual ~LossMonitor();\\n', '公有成员函数\\n', 'GetLossPoints\\n', '  const std::vector<GraphPoint> &GetLossPoints();\\n', '获取训练损失数据。\\n', '\\n', '返回值\\n', '\\n', '包含GraphPoint数据的vector，训练的损失数据。\\n', '\\n', 'LRScheduler\\n', '#include <lr_scheduler.h>\\n', '\\n', 'MetricsMindSpore Lite训练学习率调度类。\\n', '\\n', '构造函数和析构函数\\n', '  explicit LRScheduler(LR_Lambda lambda_func, void *lr_cb_data = nullptr, int step = 1);\\n', '  virtual ~LRScheduler();\\n', 'StepLRLambda\\n', '#include <lr_scheduler.h>\\n', '\\n', '一个结构体。StepLRLambda定义了训练学习率的一组参数。\\n', '\\n', '公有属性\\n', 'step_size\\n', 'step_size\\n', 'int 类型变量。学习率衰减步长。\\n', '\\n', 'gamma\\n', 'gamma\\n', 'float 类型变量。学习率衰减因子。\\n', '\\n', 'MultiplicativeLRLambda\\n', '#include <lr_scheduler.h>\\n', '\\n', '每个epoch将学习率乘以一个因子。\\n', '\\n', 'using LR_Lambda = std::function<int(float *lr, int epoch, void *cb_data)>;\\n', 'int MultiplicativeLRLambda(float *lr, int epoch, void *multiplication);\\n', '学习率更新。\\n', '\\n', '参数\\n', '\\n', 'lr: 学习率。\\n', '\\n', 'epoch: 迭代轮数。\\n', '\\n', 'multiplication: 更新方式。\\n', '\\n', '返回值\\n', '\\n', 'int类型返回值，表示是否更新，DONT_UPDATE_LR为0表示不更新，UPDATE_LR为1表示更新。\\n', '\\n', 'constexpr int DONT_UPDATE_LR = 0;\\n', 'constexpr int UPDATE_LR = 1;\\n', 'TimeMonitor\\n', '#include <time_monitor.h>\\n', '\\n', 'MetricsMindSpore Lite训练时间监测类。\\n', '\\n', '析构函数\\n', '  virtual ~TimeMonitor() = default;\\n', '公有成员函数\\n', 'EpochBegin\\n', '  void EpochBegin(const TrainCallBackData &cb_data) override;\\n', '每轮迭代前调用。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', '返回值\\n', '\\n', 'CallbackRetValue，表示是否在训练中继续循环。\\n', '\\n', 'EpochEnd\\n', '  CallbackRetValue EpochEnd(const TrainCallBackData &cb_data) override;\\n', '每轮迭代后调用。\\n', '\\n', '参数\\n', '\\n', 'cb_data: 回调参数。\\n', '\\n', '返回值\\n', '\\n', 'CallbackRetValue，表示是否在训练中继续循环。\\n', '\\n', 'TrainAccuracy\\n', '#include <train_accuracy.h>\\n', '\\n', 'MetricsMindSpore Lite训练学习率调度类。\\n', '\\n', '构造函数和析构函数\\n', 'explicit TrainAccuracy(int print_every_n = INT_MAX, int accuracy_metrics = METRICS_CLASSIFICATION, const std::vector<int> &input_indexes = {1}, const std::vector<int> &output_indexes = {0});\\n', 'virtual ~TrainAccuracy();\\n', '参数\\n', '\\n', 'print_every_n: 间隔print_every_n步打印一次。\\n', '\\n', 'accuracy_metrics: 精度指标，默认值为METRICS_CLASSIFICATION表示0，METRICS_MULTILABEL表示1。\\n', '\\n', 'input_indexes: 输入索引。\\n', '\\n', 'output_indexes: 输出索引。\\n', '\\n', 'constexpr int METRICS_CLASSIFICATION = 0;\\n', 'constexpr int METRICS_MULTILABEL = 1;\\n', 'GetAccuracyPoints\\n', '  const std::vector<GraphPoint> &GetAccuracyPoints();\\n', '获取训练精度。\\n', '\\n', '返回值\\n', '\\n', '包含GraphPoint的vector，训练精度数据。\\n', '\\n', 'CharVersion\\n', '#include <types.h>\\n', '\\n', 'std::vector<char> CharVersion();\\n', '全局方法，用于获取版本的字符vector。\\n', '\\n', '返回值\\n', '\\n', 'MindSpore Lite版本的字符vector。\\n', '\\n', 'Version\\n', '#include <types.h>\\n', '\\n', 'std::string Version()\\n', '全局方法，用于获取版本的字符串。\\n', '\\n', '返回值\\n', '\\n', 'MindSpore Lite版本的字符串。\\n', '\\n', 'Allocator\\n', '#include <allocator.h>\\n', '\\n', '内存管理基类。\\n', '\\n', '析构函数\\n', 'virtual ~Allocator()\\n', '析构函数。\\n', '\\n', '公有成员函数\\n', 'Malloc\\n', 'virtual void *Malloc(size_t size)\\n', '内存分配。\\n', '\\n', '参数\\n', '\\n', 'size: 要分配的内存大小，单位为Byte。\\n', '\\n', 'virtual void *Malloc(size_t weight, size_t height, DataType type)\\n', 'Image格式内存分配。\\n', '\\n', '参数\\n', '\\n', 'weight: 要分配的Image格式内存的宽度。\\n', '\\n', 'height: 要分配的Image格式内存的高度。\\n', '\\n', 'type: 要分配的Image格式内存的数据类型。\\n', '\\n', 'Free\\n', 'virtual void *Free(void *ptr)\\n', '内存释放。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要释放的内存地址，该值由Malloc分配。\\n', '\\n', 'RefCount\\n', 'virtual int RefCount(void *ptr)\\n', '返回分配内存的引用计数。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要操作的内存地址，该值由Malloc分配。\\n', '\\n', 'SetRefCount\\n', 'virtual int SetRefCount(void *ptr, int ref_count)\\n', '设置分配内存的引用计数。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要操作的内存地址，该值由Malloc分配。\\n', '\\n', 'ref_count: 引用计数值。\\n', '\\n', 'DecRefCount\\n', 'virtual int DecRefCount(void *ptr, int ref_count)\\n', '分配的内存引用计数减一。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要操作的内存地址，该值由Malloc分配。\\n', '\\n', 'ref_count: 引用计数值。\\n', '\\n', 'IncRefCount\\n', 'virtual int IncRefCount(void *ptr, int ref_count)\\n', '分配的内存引用计数加一。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要操作的内存地址，该值由Malloc分配。\\n', '\\n', 'ref_count: 引用计数值。\\n', '\\n', 'Create\\n', 'static std::shared_ptr<Allocator> Create()\\n', '创建默认的内存分配器。\\n', '\\n', 'Prepare\\n', 'virtual void *Prepare(void *ptr)\\n', '对分配的内存进行预处理。\\n', '\\n', '参数\\n', '\\n', 'ptr: 要操作的内存地址，该值由Malloc分配。\\n', '\\n', '保护的数据成员\\n', 'aligned_size_\\n', '内存对齐的字节数。\\n', '\\n', 'Status\\n', '#include <status.h>\\n', '\\n', '构造函数和析构函数\\n', 'Status();\\n', 'inline Status(enum StatusCode status_code, const std::string &status_msg = \"\");\\n', 'inline Status(const StatusCode code, int line_of_code, const char *file_name, const std::string &extra = \"\");\\n', '~Status() = default;\\n', 'Prepare\\n', 'enum StatusCode StatusCode() const;\\n', '获取状态码。\\n', '\\n', '返回值\\n', '\\n', '状态码。\\n', '\\n', 'ToString\\n', 'inline std::string ToString() const;\\n', '状态码转成字符串。\\n', '\\n', '返回值\\n', '\\n', '状态码的字符串。\\n', '\\n', 'GetLineOfCode\\n', 'int GetLineOfCode() const;\\n', '获取代码行数。\\n', '\\n', '返回值\\n', '\\n', '代码行数。\\n', '\\n', 'GetErrDescription\\n', 'inline std::string GetErrDescription() const;\\n', '获取错误描述字符串。\\n', '\\n', '返回值\\n', '\\n', '错误描述字符串。\\n', '\\n', 'SetErrDescription\\n', 'inline std::string SetErrDescription(const std::string &err_description);\\n', '配置错误描述字符串。\\n', '\\n', '参数\\n', '\\n', 'err_description: 错误描述字符串。\\n', '\\n', '返回值\\n', '\\n', '状态信息字符串。\\n', '\\n', 'operator<<(std::ostream &os, const Status &s)\\n', 'friend std::ostream &operator<<(std::ostream &os, const Status &s);\\n', '状态信息写到输出流。\\n', '\\n', '参数\\n', '\\n', 'os: 输出流。\\n', '\\n', 's: 状态类。\\n', '\\n', '返回值\\n', '\\n', '输出流。\\n', '\\n', 'operator==(const Status &other)\\n', 'bool operator==(const Status &other) const;\\n', '判断是否与另一个Status相等。\\n', '\\n', '参数\\n', '\\n', 'other: 另一个Status。\\n', '\\n', '返回值\\n', '\\n', '是否与另一个Status相等。\\n', '\\n', 'operator==(enum StatusCode other_code)\\n', 'bool operator==(enum StatusCode other_code) const;\\n', '判断是否与一个StatusCode相等。\\n', '\\n', '参数\\n', '\\n', 'other_code: 一个StatusCode。\\n', '\\n', '返回值\\n', '\\n', '是否与一个StatusCode相等。\\n', '\\n', 'operator!=(enum StatusCode other_code)\\n', 'bool operator!=(enum StatusCode other_code) const;\\n', '判断是否与一个StatusCode不等。\\n', '\\n', '参数\\n', '\\n', 'other_code: 一个StatusCode。\\n', '\\n', '返回值\\n', '\\n', '是否与一个StatusCode不等。\\n', '\\n', 'operator bool()\\n', 'explicit operator bool() const;\\n', '重载bool操作。\\n', '\\n', 'explicit operator int() const\\n', 'explicit operator int() const;\\n', '重载int操作。\\n', '\\n', 'OK\\n', 'static Status OK();\\n', '获取kSuccess的状态码。\\n', '\\n', '返回值\\n', '\\n', 'StatusCode::kSuccess。\\n', '\\n', 'IsOk\\n', 'bool IsOk() const;\\n', '判断是否是kSuccess的状态码。\\n', '\\n', '返回值\\n', '\\n', '是否是kSuccess。\\n', '\\n', 'IsError\\n', 'bool IsError() const;\\n', '判断是否不是kSuccess的状态码。\\n', '\\n', '返回值\\n', '\\n', '是否不是kSuccess。\\n', '\\n', 'CodeAsString\\n', 'static inline std::string CodeAsString(enum StatusCode c);\\n', '获取StatusCode对应的字符串。\\n', '\\n', '参数\\n', '\\n', 'c: 状态码枚举值。\\n', '\\n', '返回值\\n', '\\n', '状态码对应的字符串。\\n', '\\n', 'Graph\\n', '#include <graph.h>\\n', '\\n', '构造函数和析构函数\\n', '  Graph();\\n', '  explicit Graph(const std::shared_ptr<GraphData> &graph_data);\\n', '  explicit Graph(std::shared_ptr<GraphData> &&graph_data);\\n', '  explicit Graph(std::nullptr_t);\\n', '  ~Graph();\\n', '参数\\n', '\\n', 'graph_data: 输出通道数。\\n', '\\n', '公有成员函数\\n', 'ModelType\\n', '  enum ModelType ModelType() const;\\n', '获取模型类型。\\n', '\\n', '返回值\\n', '\\n', '模型类型。\\n', '\\n', 'operator==(std::nullptr_t)\\n', '  bool operator==(std::nullptr_t) const;\\n', '判断是否为空指针。\\n', '\\n', '返回值\\n', '\\n', '是否为空指针。\\n', '\\n', 'operator!=(std::nullptr_t)\\n', '  bool operator!=(std::nullptr_t) const;\\n', '判断是否为非空指针。\\n', '\\n', '返回值\\n', '\\n', '是否为非空指针。\\n', '\\n', 'CellBase\\n', '#include <cell.h>\\n', '\\n', '构造函数和析构函数\\n', '  CellBase() = default;\\n', '  virtual ~CellBase() = default;\\n', '公有成员函数\\n', 'Clone\\n', '  virtual std::shared_ptr<CellBase> Clone() const = 0;\\n', '拷贝一份自身的副本。\\n', '\\n', '返回值\\n', '\\n', '指向副本的指针。\\n', '\\n', 'Cell\\n', '#include <cell.h>\\n', '\\n', '析构函数\\n', '  virtual ~Cell() = default;\\n', '公有成员函数\\n', 'Clone\\n', '  std::shared_ptr<CellBase> Clone() const;\\n', '拷贝一份自身的副本。\\n', '\\n', '返回值\\n', '\\n', '指向副本的指针。\\n', '\\n', 'GraphCell\\n', '#include <cell.h>\\n', '\\n', '构造函数和析构函数\\n', '  GraphCell() = default;\\n', '  ~GraphCell() override = default;\\n', '  explicit GraphCell(const Graph &);\\n', '  explicit GraphCell(Graph &&);\\n', '  explicit GraphCell(const std::shared_ptr<Graph> &);\\n', '公有成员函数\\n', 'GetGraph\\n', '  const std::shared_ptr<Graph> &GetGraph() const { return graph_; }\\n', '获取Graph指针。\\n', '\\n', '返回值\\n', '\\n', '指向Graph的指针。\\n', '\\n', 'RunnerConfig\\n', '#include <model_parallel_runner.h>\\n', '\\n', 'RunnerConfig定义了ModelParallelRunner中使用的配置选项参数。\\n', '\\n', '构造函数和析构函数\\n', 'RunnerConfig();\\n', '~RunnerConfig();\\n', '公有成员函数\\n', 'SetWorkersNum\\n', 'void SetWorkersNum(int32_t workers_num);\\n', '设置RunnerConfig的worker的个数。\\n', '\\n', '参数\\n', '\\n', 'workers_num: worker的数量。\\n', '\\n', 'GetWorkersNum\\n', 'int32_t GetWorkersNum() const;\\n', '获取RunnerConfig配置的上下文参数。\\n', '\\n', '返回值\\n', '\\n', 'RunnerConfig类中配置的worker数量。\\n', '\\n', 'SetContext\\n', 'void SetContext(const std::shared_ptr<Context> &context);\\n', '设置RunnerConfig的context参数。\\n', '\\n', '参数\\n', '\\n', 'context: worker上下文配置。\\n', '\\n', 'GetContext\\n', 'std::shared_ptr<Context> GetContext() const;\\n', '获取RunnerConfig配置的上下文参数。\\n', '\\n', '返回值\\n', '\\n', '上下文配置类Context对象。\\n', '\\n', 'ModelParallelRunner\\n', '#include <model_parallel_runner.h>\\n', '\\n', 'ModelParallelRunner定义了MindSpore的多个Model以及并发策略，便于多个Model的调度与管理。\\n', '\\n', '构造函数和析构函数\\n', 'ModelParallelRunner();\\n', '~ModelParallelRunner();\\n', '公有成员函数\\n', 'Init\\n', 'Status Init(const std::string &model_path, const std::shared_ptr<RunnerConfig> &runner_config = nullptr);\\n', '根据路径读取加载模型，生成一个或者多个模型，并将所有模型编译至可在Device上运行的状态。\\n', '\\n', '参数\\n', '\\n', 'model_path: 模型文件路径。\\n', '\\n', 'runner_config: 一个RunnerConfig类。定义了并发推理模型的配置参数。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'Predict\\n', 'Status Predict(const std::vector<MSTensor> &inputs, std::vector<MSTensor> *outputs,\\n', '                 const MSKernelCallBack &before = nullptr, const MSKernelCallBack &after = nullptr);\\n', '并发推理模型。\\n', '\\n', '参数\\n', '\\n', 'inputs: 模型输入按顺序排列的vector。\\n', '\\n', 'outputs: 输出参数，按顺序排列的vector的指针，模型输出会按顺序填入该容器。\\n', '\\n', 'before: 一个MSKernelCallBack 结构体。定义了运行每个节点之前调用的回调函数。\\n', '\\n', 'after: 一个MSKernelCallBack 结构体。定义了运行每个节点之后调用的回调函数。\\n', '\\n', '返回值\\n', '\\n', '状态码类Status对象，可以使用其公有函数StatusCode或ToString函数来获取具体错误码及错误信息。\\n', '\\n', 'GetInputs\\n', 'std::vector<MSTensor> GetInputs();\\n', '获取模型所有输入张量。\\n', '\\n', '返回值\\n', '\\n', '包含模型所有输入张量的容器类型变量。\\n', '\\n', 'GetOutputs\\n', 'std::vector<MSTensor> GetOutputs();\\n', '获取模型所有输出张量。\\n', '\\n', '返回值\\n', '\\n', '包含模型所有输出张量的容器类型变量。']"}
{"index": {"_index": "r1.7-c++-api", "_id": "样例.txt"}}
{"file_link": "https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/lite_cpp_example.html", "text_entry": "['样例\\n', '极简Demo↗\\n', '基于JNI接口的Android应用开发↗\\n', '高阶用法↗']"}
